{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About This Course \u00b6 Go to the setup page for instructions on how to setup your computer for this course.","title":"About"},{"location":"#about-this-course","text":"Go to the setup page for instructions on how to setup your computer for this course.","title":"About This Course"},{"location":"data/","text":"Data Sets \u00b6 Berkley DeepDrive \u00b6 Data from Berkley DeepDrive is found in data/external/bdd . License \u00b6 Copyright \u00a92018. The Regents of the University of California (Regents). All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement; and permission use, copy, modify and distribute this software for commercial purposes (such rights not subject to transfer) to BDD member and its affiliates, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, otl@berkeley.edu , http://ipira.berkeley.edu/industry-info for commercial licensing opportunities. IN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS. Enron Emails \u00b6 The folder data/external/enron contains a partial copy of the Enron Email dataset . Large Movie Review \u00b6 Large Movie Review Dataset contains 25,000 movie reviews from IMDB and can be found in data/external/imdb . OpenFlights \u00b6 License \u00b6 The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation. Data \u00b6 OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available Airports \u00b6 Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes Airlines \u00b6 Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE Routes \u00b6 Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key Planes \u00b6 Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes Countries \u00b6 Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes. Tidynomicon \u00b6 Data copied from the Tidynomicon Github repository . License \u00b6 This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Data Sets"},{"location":"data/#data-sets","text":"","title":"Data Sets"},{"location":"data/#berkley-deepdrive","text":"Data from Berkley DeepDrive is found in data/external/bdd .","title":"Berkley DeepDrive"},{"location":"data/#license","text":"Copyright \u00a92018. The Regents of the University of California (Regents). All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement; and permission use, copy, modify and distribute this software for commercial purposes (such rights not subject to transfer) to BDD member and its affiliates, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, otl@berkeley.edu , http://ipira.berkeley.edu/industry-info for commercial licensing opportunities. IN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.","title":"License"},{"location":"data/#enron-emails","text":"The folder data/external/enron contains a partial copy of the Enron Email dataset .","title":"Enron Emails"},{"location":"data/#large-movie-review","text":"Large Movie Review Dataset contains 25,000 movie reviews from IMDB and can be found in data/external/imdb .","title":"Large Movie Review"},{"location":"data/#openflights","text":"","title":"OpenFlights"},{"location":"data/#license_1","text":"The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation.","title":"License"},{"location":"data/#data","text":"OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available","title":"Data"},{"location":"data/#airports","text":"Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes","title":"Airports"},{"location":"data/#airlines","text":"Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE","title":"Airlines"},{"location":"data/#routes","text":"Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key","title":"Routes"},{"location":"data/#planes","text":"Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes","title":"Planes"},{"location":"data/#countries","text":"Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes.","title":"Countries"},{"location":"data/#tidynomicon","text":"Data copied from the Tidynomicon Github repository .","title":"Tidynomicon"},{"location":"data/#license_2","text":"This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"faq/neural-networks/","text":"Number of Training Epochs \u00b6 Question What is the optimal number of training epochs to use when training a model? A training epoch refers to one sweep through the entire training set. From Neural Networks Part 3: Learning and Evaluation Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters. Number of Hidden Units and Layers \u00b6 Question What is the optimal number of hidden units and layers to use when training a model? From Elements of Statistical learning Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used. Typically the number of hidden units is somewhere in the range of 5 to 100, with the number increasing with the number of inputs and number of training cases. It is most common to put down a reasonably large number of units and train them with regularization. Some researchers use cross-validation to estimate the optimal number, but this seems unnecessary if cross-validation is used to estimate the regularization parameter. Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution. Convolutional Networks \u00b6 See layer sizing patterns for information on the number of layers for convolutional networks. From Neural Networks Part 1: Setting up the Architecture To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. From Convolutional Neural Networks In practice: use whatever works best on ImageNet. If you\u2019re feeling a bit of a fatigue in thinking about the architectural decisions, you\u2019ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as \u201cdon\u2019t be a hero\u201d: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. From Deep Residual Learning for Image Recognition Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difficulty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6). But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future. From Neural Networks Part 1: Setting up the Architecture The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting. Densely Connected Networks \u00b6 From Neural Networks Part 1: Setting up the Architecture As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain. Large Number of Categories \u00b6 Question What is the best practices when trying to train models to classify large numbers of categories? From Elements of Statistical learning With N N observations, p p predictors, M M hidden units and L L training epochs, a neural network fit typically requires O(NpML) O(NpML) operations. From Neural Networks Part 2: Setting up the Data and the Loss Problem: Large number of classes. When the set of labels is very large (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), computing the full softmax probabilities becomes expensive. For certain applications, approximate versions are popular. For instance, it may be helpful to use Hierarchical Softmax in natural language processing tasks (see one explanation here (pdf)). The hierarchical softmax decomposes words as labels in a tree. Each label is then represented as a path along the tree, and a Softmax classifier is trained at every node of the tree to disambiguate between the left and right branch. The structure of the tree strongly impacts the performance and is generally problem-dependent. Dropout and Regularization \u00b6 Question What is the best practice for using dropout and regularization? From Neural Networks Part 1: Setting up the Architecture . . . there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods to control overfitting instead of the number of neurons. To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. From Neural Networks Part 2: Setting up the Data and the Loss In practice: It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of p=0.5 is a reasonable default, but this can be tuned on validation data. From Dropout: A Simple Way to Prevent Neural Networks from Overfitting It is to be expected that dropping units will reduce the capacity of a neural network. If n is the number of hidden units in any layer and p is the probability of retaining a unit, then instead of n hidden units, only pn units will be present after dropout, in expectation. Moreover, this set of pn units will be different each time and the units are not allowed to build co-adaptations freely. Therefore, if an n -sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least n=p units. We found this to be a useful heuristic for setting the number of hidden units in both convolutional and fully connected networks. Hyperparameter Selection and Optimization \u00b6 Question How do you select and optimize Hyperparameters? See hyperparameter optimization and Random Search for Hyper-Parameter Optimization for more information. Here is a short excerpt: Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs) You can also look into tools like hyperopt","title":"Neural Networks"},{"location":"faq/neural-networks/#number-of-training-epochs","text":"Question What is the optimal number of training epochs to use when training a model? A training epoch refers to one sweep through the entire training set. From Neural Networks Part 3: Learning and Evaluation Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters.","title":"Number of Training Epochs"},{"location":"faq/neural-networks/#number-of-hidden-units-and-layers","text":"Question What is the optimal number of hidden units and layers to use when training a model? From Elements of Statistical learning Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used. Typically the number of hidden units is somewhere in the range of 5 to 100, with the number increasing with the number of inputs and number of training cases. It is most common to put down a reasonably large number of units and train them with regularization. Some researchers use cross-validation to estimate the optimal number, but this seems unnecessary if cross-validation is used to estimate the regularization parameter. Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.","title":"Number of Hidden Units and Layers"},{"location":"faq/neural-networks/#convolutional-networks","text":"See layer sizing patterns for information on the number of layers for convolutional networks. From Neural Networks Part 1: Setting up the Architecture To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. From Convolutional Neural Networks In practice: use whatever works best on ImageNet. If you\u2019re feeling a bit of a fatigue in thinking about the architectural decisions, you\u2019ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as \u201cdon\u2019t be a hero\u201d: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. From Deep Residual Learning for Image Recognition Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difficulty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6). But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future. From Neural Networks Part 1: Setting up the Architecture The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.","title":"Convolutional Networks"},{"location":"faq/neural-networks/#densely-connected-networks","text":"From Neural Networks Part 1: Setting up the Architecture As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.","title":"Densely Connected Networks"},{"location":"faq/neural-networks/#large-number-of-categories","text":"Question What is the best practices when trying to train models to classify large numbers of categories? From Elements of Statistical learning With N N observations, p p predictors, M M hidden units and L L training epochs, a neural network fit typically requires O(NpML) O(NpML) operations. From Neural Networks Part 2: Setting up the Data and the Loss Problem: Large number of classes. When the set of labels is very large (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), computing the full softmax probabilities becomes expensive. For certain applications, approximate versions are popular. For instance, it may be helpful to use Hierarchical Softmax in natural language processing tasks (see one explanation here (pdf)). The hierarchical softmax decomposes words as labels in a tree. Each label is then represented as a path along the tree, and a Softmax classifier is trained at every node of the tree to disambiguate between the left and right branch. The structure of the tree strongly impacts the performance and is generally problem-dependent.","title":"Large Number of Categories"},{"location":"faq/neural-networks/#dropout-and-regularization","text":"Question What is the best practice for using dropout and regularization? From Neural Networks Part 1: Setting up the Architecture . . . there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods to control overfitting instead of the number of neurons. To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. From Neural Networks Part 2: Setting up the Data and the Loss In practice: It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of p=0.5 is a reasonable default, but this can be tuned on validation data. From Dropout: A Simple Way to Prevent Neural Networks from Overfitting It is to be expected that dropping units will reduce the capacity of a neural network. If n is the number of hidden units in any layer and p is the probability of retaining a unit, then instead of n hidden units, only pn units will be present after dropout, in expectation. Moreover, this set of pn units will be different each time and the units are not allowed to build co-adaptations freely. Therefore, if an n -sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least n=p units. We found this to be a useful heuristic for setting the number of hidden units in both convolutional and fully connected networks.","title":"Dropout and Regularization"},{"location":"faq/neural-networks/#hyperparameter-selection-and-optimization","text":"Question How do you select and optimize Hyperparameters? See hyperparameter optimization and Random Search for Hyper-Parameter Optimization for more information. Here is a short excerpt: Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs) You can also look into tools like hyperopt","title":"Hyperparameter Selection and Optimization"},{"location":"lessons/","text":"Overview \u00b6","title":"Overview"},{"location":"lessons/#overview","text":"","title":"Overview"},{"location":"lessons/10-week/","text":"Documentation in Progress Check back soon for more updates.","title":"Index"},{"location":"lessons/10-week/week01/","text":"Week 1 \u00b6 To paraphrase the late Douglas Adams: Big data is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think your collection of movies, pictures, and music is big, but that's just peanuts to big data. Big Data is big in two distinct ways. First, as the name suggests, Big Data is about how to deal with large amounts of data. Tech giants like Google and Facebook store exabytes of data . While multiple exabytes of data is an impressive amount of data, it is nowhere near the theoretical limits . Second, Big Data is a wide area of study that spans a wide range of technologies and concepts. Because of the size and rapid rate of change of the subject, we will only be able to cover a small fraction of Big Data topics in this course. In this course, we will focus on two main areas: the design of data-driven systems and deep learning algorithms. The first area, the design of data-driven systems, takes a high-level look into how the different components of Big Data systems fit together. The second area, deep learning, focuses on a specific approach to extracting information from large, usually unstructured datasets. Objectives \u00b6 After completing this week, you should be able to: Setup a development environment PySpark, Keras, and TensorFlow and run a simple proof of concept Explain how reliability, scalability, and maintainability impacts data-driven systems Summarize how artificial intelligence, machine learning, and deep learning relate to one another Determine what problems deep learning and big data help solve Readings \u00b6 Read chapter 1 in Designing Data-Intensive Applications Read chapter 1 in Deep Learning with Python Visit DSC 650 website and follow the getting started instructions to setup your development environment Watch CPU vs GPU What's the Difference? What's the Difference? Watch How Much Information is in the Universe? | Space Time Weekly Resources \u00b6 Backblaze Hard Drive Stats DSC 650 Website DSC 650 Github Repository CPU vs GPU What's the Difference? What's the Difference? How Much Information is in the Universe? | Space Time Assignment 1 \u00b6 Assignment 1.1 \u00b6 Visit [DSC 650 website][dsc650] and follow the instructions for getting started. To demonstrate your environment is in working order, run the examples in the examples folder, and copy the output to the dsc650/assignment01/logs folder. a. Run Keras MNIST MLP Example \u00b6 If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/mnist_mlp.py > logs/keras-mnist.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. b. Run PySpark Example \u00b6 If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/pi.py > logs/spark-pi.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. Assignment 1.2 \u00b6 For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at dsc650/assignments/assignment01/Assignment 01.md . a. Data Sizes \u00b6 Provide estimates for the size of various data items. Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB Assume all videos are 30 frames per second HEVC stands for High Efficiency Video Coding See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions. b. Scaling \u00b6 Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items. Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS). By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. See the Snappy Github repository for estimates of Snappy's performance. Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day. Assume that 75% of those items are 1024x768 PNG photos. YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute. For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second. c. Reliability \u00b6 Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics . # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ?? d. Latency \u00b6 Provide estimates of the one way latency for each of the following items. Please explain how you arrived at the estimates for each item by citing references or providing calculations. One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment01/ directory. Use the naming convention of assignment01_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment01_DoeJane.zip assignment01 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment01 -DestinationPath ' assignment01_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment01 \u2502 \u251c\u2500\u2500 Assignment\\ 01.md \u2502 \u2514\u2500\u2500 logs \u2502 \u251c\u2500\u2500 keras-mnist.log \u2502 \u2514\u2500\u2500 spark-pi.log Discussion \u00b6 For the first discussion, write a 250 to 750-word discussion board post about a Big Data and/or deep learning use case. Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 1"},{"location":"lessons/10-week/week01/#week-1","text":"To paraphrase the late Douglas Adams: Big data is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think your collection of movies, pictures, and music is big, but that's just peanuts to big data. Big Data is big in two distinct ways. First, as the name suggests, Big Data is about how to deal with large amounts of data. Tech giants like Google and Facebook store exabytes of data . While multiple exabytes of data is an impressive amount of data, it is nowhere near the theoretical limits . Second, Big Data is a wide area of study that spans a wide range of technologies and concepts. Because of the size and rapid rate of change of the subject, we will only be able to cover a small fraction of Big Data topics in this course. In this course, we will focus on two main areas: the design of data-driven systems and deep learning algorithms. The first area, the design of data-driven systems, takes a high-level look into how the different components of Big Data systems fit together. The second area, deep learning, focuses on a specific approach to extracting information from large, usually unstructured datasets.","title":"Week 1"},{"location":"lessons/10-week/week01/#objectives","text":"After completing this week, you should be able to: Setup a development environment PySpark, Keras, and TensorFlow and run a simple proof of concept Explain how reliability, scalability, and maintainability impacts data-driven systems Summarize how artificial intelligence, machine learning, and deep learning relate to one another Determine what problems deep learning and big data help solve","title":"Objectives"},{"location":"lessons/10-week/week01/#readings","text":"Read chapter 1 in Designing Data-Intensive Applications Read chapter 1 in Deep Learning with Python Visit DSC 650 website and follow the getting started instructions to setup your development environment Watch CPU vs GPU What's the Difference? What's the Difference? Watch How Much Information is in the Universe? | Space Time","title":"Readings"},{"location":"lessons/10-week/week01/#weekly-resources","text":"Backblaze Hard Drive Stats DSC 650 Website DSC 650 Github Repository CPU vs GPU What's the Difference? What's the Difference? How Much Information is in the Universe? | Space Time","title":"Weekly Resources"},{"location":"lessons/10-week/week01/#assignment-1","text":"","title":"Assignment 1"},{"location":"lessons/10-week/week01/#assignment-11","text":"Visit [DSC 650 website][dsc650] and follow the instructions for getting started. To demonstrate your environment is in working order, run the examples in the examples folder, and copy the output to the dsc650/assignment01/logs folder.","title":"Assignment 1.1"},{"location":"lessons/10-week/week01/#a-run-keras-mnist-mlp-example","text":"If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/mnist_mlp.py > logs/keras-mnist.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor.","title":"a.  Run Keras MNIST MLP Example"},{"location":"lessons/10-week/week01/#b-run-pyspark-example","text":"If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/pi.py > logs/spark-pi.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor.","title":"b. Run PySpark Example"},{"location":"lessons/10-week/week01/#assignment-12","text":"For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at dsc650/assignments/assignment01/Assignment 01.md .","title":"Assignment 1.2"},{"location":"lessons/10-week/week01/#a-data-sizes","text":"Provide estimates for the size of various data items. Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB Assume all videos are 30 frames per second HEVC stands for High Efficiency Video Coding See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions.","title":"a. Data Sizes"},{"location":"lessons/10-week/week01/#b-scaling","text":"Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items. Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS). By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. See the Snappy Github repository for estimates of Snappy's performance. Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day. Assume that 75% of those items are 1024x768 PNG photos. YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute. For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second.","title":"b. Scaling"},{"location":"lessons/10-week/week01/#c-reliability","text":"Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics . # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ??","title":"c. Reliability"},{"location":"lessons/10-week/week01/#d-latency","text":"Provide estimates of the one way latency for each of the following items. Please explain how you arrived at the estimates for each item by citing references or providing calculations. One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes","title":"d. Latency"},{"location":"lessons/10-week/week01/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment01/ directory. Use the naming convention of assignment01_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment01_DoeJane.zip assignment01 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment01 -DestinationPath ' assignment01_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment01 \u2502 \u251c\u2500\u2500 Assignment\\ 01.md \u2502 \u2514\u2500\u2500 logs \u2502 \u251c\u2500\u2500 keras-mnist.log \u2502 \u2514\u2500\u2500 spark-pi.log","title":"Submission Instructions"},{"location":"lessons/10-week/week01/#discussion","text":"For the first discussion, write a 250 to 750-word discussion board post about a Big Data and/or deep learning use case. Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/10-week/week02/","text":"Week 2 \u00b6 In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms. Objectives \u00b6 After completing this week, you should be able to: Query and process data using multiple paradigms including graph processing, map-reduce, and SQL Compare and contrast different data models including identifying prime use cases for different data models Demonstrate how to represent data as tensors and apply tensor mathematical operations Readings \u00b6 Read chapters 2 and 3 in Designing Data-Intensive Applications Read chapter 2 in Deep Learning with Python Weekly Resources \u00b6 TinyDB OrientDB Getting Started OrientDB Download Keras Multi-Dimensional Data as used in Tensors as used in Tensors SQL Tutorial TensorFlow Quickstart Assignment 2 \u00b6 The dsc650/assignments/assignment02 folder contains skeleton code for this assignment. Provide the code to implement the functions in the run_assignment.py file. For this assignment, we will be working with the CSV data found in the data/external/tidynomicon folder. Specifically, we will be using with the measurements.csv , person.csv , site.csv , and visited.csv files. Assignment 2.1 \u00b6 Complete the code in kvdb.py to implement a basic key-value database that saves its state to a pickle file. Use that code to create databases that store each of CSV files by key. The pickle files should be stored in the dsc650/assignments/assignment02/results/kvdb/ folder. Input File Output File Key measurements.csv measurements.pickle Composite key person.csv people.pickle person_id site.csv sites.pickle site_id visited.csv visits.pickle Composite key The measurements.csv and visited.csv have composite keys that use multiple columns. For measurements.csv those fields are visit_id , person_id , and quantity . For visited.csv those fields are visit_id and site_id . The following is an example of code that sets and gets the value using a composite key. kvdb_path = 'visits.pickle' kvdb = KVDB ( kvdb_path ) key = ( 619 , 'DR-1' ) value = dict ( visit_id = 619 , site_id = 'DR-1' , visit_date = '1927-02-08' ) kvdb . set_value ( key , value ) retrieved_value = kvdb . get_value ( key ) # Retrieved should be the same as value Assignment 2.2 \u00b6 Now we will create a simple document database using the tinydb library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in dsc650/assignments/assignment02/results/patient-info.json . You will store a document for each person in the database which should look like this. { \"person_id\" : \"dyer\" , \"personal_name\" : \"William\" , \"family_name\" : \"Dyer\" , \"visits\" : [ { \"visit_id\" : 619 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-08\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 9.82 }, { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.13 } ] }, { \"visit_id\" : 622 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-10\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 7.8 }, { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.09 } ] } ] } The dsc650/assignments/assignment02/documentdb.py file contains code that should assist you in this task. Assignment 2.3 \u00b6 Complete the code in dsc650/assignments/assignment02/objectdb to implement an object database using ZODB . You will store the database in dsc650/assignments/assignment02/results/patient-info.fs Assignment 2.4 \u00b6 In this part, you will create a SQLite database which you will store in dsc650/assignments/assignment02/results/patient-info.db . The dsc650/assignments/assignment02/rdbms.py file should contain code to assist you in the creation of this database. Assignment 2.5 \u00b6 Go to the Wikidata Query Service website and perform the following SPARQL query. #Recent Events SELECT ?event ?eventLabel ?date WHERE { # find events ?event wdt : P31 / wdt : P279 * wd : Q1190554 . # with a point in time or start date OPTIONAL { ?event wdt : P585 ?date . } OPTIONAL { ?event wdt : P580 ?date . } # but at least one of those FILTER ( BOUND ( ?date ) && DATATYPE ( ?date ) = xsd : dateTime ). # not in the future, and not more than 31 days ago BIND ( NOW () - ?date AS ?distance ). FILTER ( 0 <= ?distance && ?distance < 31 ). # and get a label as well OPTIONAL { ?event rdfs : label ?eventLabel . FILTER ( LANG ( ?eventLabel ) = \"en\" ). } } # limit to 10 results so we don't timeout LIMIT 10 Modify the query so that the column order is date , event , and eventLabel instead of event , eventLabel , and date. Download the results as a JSON file and copy the results to dsc650/assignments/assignment02/results/wikidata-query.json`. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment02/ directory. Use the naming convention of assignment02_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment02_DoeJane.zip assignment02 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment02 -DestinationPath ' assignment02_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment02 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 documentdb.py \u2502 \u251c\u2500\u2500 kvdb.py \u2502 \u251c\u2500\u2500 objectdb.py \u2502 \u251c\u2500\u2500 rdbms.py \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u251c\u2500\u2500 kvdb \u2502 \u2502 \u2502 \u251c\u2500\u2500 measurements.pickle \u2502 \u2502 \u2502 \u251c\u2500\u2500 people.pickle \u2502 \u2502 \u2502 \u251c\u2500\u2500 sites.pickle \u2502 \u2502 \u2502 \u2514\u2500\u2500 visits.pickle \u2502 \u2502 \u251c\u2500\u2500 patient-info.db \u2502 \u2502 \u251c\u2500\u2500 patient-info.fs \u2502 \u2502 \u251c\u2500\u2500 patient-info.json \u2502 \u2502 \u2514\u2500\u2500 wikidata-query.json \u2502 \u251c\u2500\u2500 run_assignment.py \u2502 \u2514\u2500\u2500 util.py Discussion \u00b6 For this discussion, write a 250 to 750-word discussion board post about use cases from different data models. As an example, how could you use a graph database in one of your professional or personal projects? Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 2"},{"location":"lessons/10-week/week02/#week-2","text":"In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms.","title":"Week 2"},{"location":"lessons/10-week/week02/#objectives","text":"After completing this week, you should be able to: Query and process data using multiple paradigms including graph processing, map-reduce, and SQL Compare and contrast different data models including identifying prime use cases for different data models Demonstrate how to represent data as tensors and apply tensor mathematical operations","title":"Objectives"},{"location":"lessons/10-week/week02/#readings","text":"Read chapters 2 and 3 in Designing Data-Intensive Applications Read chapter 2 in Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week02/#weekly-resources","text":"TinyDB OrientDB Getting Started OrientDB Download Keras Multi-Dimensional Data as used in Tensors as used in Tensors SQL Tutorial TensorFlow Quickstart","title":"Weekly Resources"},{"location":"lessons/10-week/week02/#assignment-2","text":"The dsc650/assignments/assignment02 folder contains skeleton code for this assignment. Provide the code to implement the functions in the run_assignment.py file. For this assignment, we will be working with the CSV data found in the data/external/tidynomicon folder. Specifically, we will be using with the measurements.csv , person.csv , site.csv , and visited.csv files.","title":"Assignment 2"},{"location":"lessons/10-week/week02/#assignment-21","text":"Complete the code in kvdb.py to implement a basic key-value database that saves its state to a pickle file. Use that code to create databases that store each of CSV files by key. The pickle files should be stored in the dsc650/assignments/assignment02/results/kvdb/ folder. Input File Output File Key measurements.csv measurements.pickle Composite key person.csv people.pickle person_id site.csv sites.pickle site_id visited.csv visits.pickle Composite key The measurements.csv and visited.csv have composite keys that use multiple columns. For measurements.csv those fields are visit_id , person_id , and quantity . For visited.csv those fields are visit_id and site_id . The following is an example of code that sets and gets the value using a composite key. kvdb_path = 'visits.pickle' kvdb = KVDB ( kvdb_path ) key = ( 619 , 'DR-1' ) value = dict ( visit_id = 619 , site_id = 'DR-1' , visit_date = '1927-02-08' ) kvdb . set_value ( key , value ) retrieved_value = kvdb . get_value ( key ) # Retrieved should be the same as value","title":"Assignment 2.1"},{"location":"lessons/10-week/week02/#assignment-22","text":"Now we will create a simple document database using the tinydb library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in dsc650/assignments/assignment02/results/patient-info.json . You will store a document for each person in the database which should look like this. { \"person_id\" : \"dyer\" , \"personal_name\" : \"William\" , \"family_name\" : \"Dyer\" , \"visits\" : [ { \"visit_id\" : 619 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-08\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 9.82 }, { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.13 } ] }, { \"visit_id\" : 622 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-10\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 7.8 }, { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.09 } ] } ] } The dsc650/assignments/assignment02/documentdb.py file contains code that should assist you in this task.","title":"Assignment 2.2"},{"location":"lessons/10-week/week02/#assignment-23","text":"Complete the code in dsc650/assignments/assignment02/objectdb to implement an object database using ZODB . You will store the database in dsc650/assignments/assignment02/results/patient-info.fs","title":"Assignment 2.3"},{"location":"lessons/10-week/week02/#assignment-24","text":"In this part, you will create a SQLite database which you will store in dsc650/assignments/assignment02/results/patient-info.db . The dsc650/assignments/assignment02/rdbms.py file should contain code to assist you in the creation of this database.","title":"Assignment 2.4"},{"location":"lessons/10-week/week02/#assignment-25","text":"Go to the Wikidata Query Service website and perform the following SPARQL query. #Recent Events SELECT ?event ?eventLabel ?date WHERE { # find events ?event wdt : P31 / wdt : P279 * wd : Q1190554 . # with a point in time or start date OPTIONAL { ?event wdt : P585 ?date . } OPTIONAL { ?event wdt : P580 ?date . } # but at least one of those FILTER ( BOUND ( ?date ) && DATATYPE ( ?date ) = xsd : dateTime ). # not in the future, and not more than 31 days ago BIND ( NOW () - ?date AS ?distance ). FILTER ( 0 <= ?distance && ?distance < 31 ). # and get a label as well OPTIONAL { ?event rdfs : label ?eventLabel . FILTER ( LANG ( ?eventLabel ) = \"en\" ). } } # limit to 10 results so we don't timeout LIMIT 10 Modify the query so that the column order is date , event , and eventLabel instead of event , eventLabel , and date. Download the results as a JSON file and copy the results to dsc650/assignments/assignment02/results/wikidata-query.json`.","title":"Assignment 2.5"},{"location":"lessons/10-week/week02/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment02/ directory. Use the naming convention of assignment02_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment02_DoeJane.zip assignment02 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment02 -DestinationPath ' assignment02_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment02 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 documentdb.py \u2502 \u251c\u2500\u2500 kvdb.py \u2502 \u251c\u2500\u2500 objectdb.py \u2502 \u251c\u2500\u2500 rdbms.py \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u251c\u2500\u2500 kvdb \u2502 \u2502 \u2502 \u251c\u2500\u2500 measurements.pickle \u2502 \u2502 \u2502 \u251c\u2500\u2500 people.pickle \u2502 \u2502 \u2502 \u251c\u2500\u2500 sites.pickle \u2502 \u2502 \u2502 \u2514\u2500\u2500 visits.pickle \u2502 \u2502 \u251c\u2500\u2500 patient-info.db \u2502 \u2502 \u251c\u2500\u2500 patient-info.fs \u2502 \u2502 \u251c\u2500\u2500 patient-info.json \u2502 \u2502 \u2514\u2500\u2500 wikidata-query.json \u2502 \u251c\u2500\u2500 run_assignment.py \u2502 \u2514\u2500\u2500 util.py","title":"Submission Instructions"},{"location":"lessons/10-week/week02/#discussion","text":"For this discussion, write a 250 to 750-word discussion board post about use cases from different data models. As an example, how could you use a graph database in one of your professional or personal projects? Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/10-week/week03/","text":"Week 3 \u00b6 In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes. Objectives \u00b6 After completing this week, you should be able to: Compare indexing algorithms including hash indexes and B-Trees Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution Readings \u00b6 Read chapters 3 and 4 in Designing Data-Intensive Applications Weekly Resources \u00b6 Apache Arrow Apache Parquet Apache Thrift Apache Avro JSON Schema Protocol Buffers Assignment 3 \u00b6 For this assignment, you will be working with data from OpenFlights . This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in data/external/openflights/ . For this assignment, you will use a dataset derived from that original data. You can find this data in data/processed/openflights/routes.jsonl.gz . The data is compressed with gzip and encoded in the JSON Lines format . Each line represents a single airline route. The dsc650/assignments/assignment03 directory contains placeholder code and data outputs for this assignment. Assignment 3.1 \u00b6 In the first part of the assignment, you will be creating schemas for the route data and encoding the routes.jsonl.gz using Protocol Buffers, Avro, and Parquet. a. JSON Schema \u00b6 Create a JSON Schema in the schemas/routes-schema.json file to describe a route and validate the data in routes.jsonl.gz using the jsonschema library. b. Avro \u00b6 Create an Avro schema in the schemas/routes.avsc file to describe a route and validate the data in routes.jsonl.gz . Use Avro's Python library or fastavro library to create results/routes.avro with the schema you created. Do not use any compression on the output at this stage. c. Parquet \u00b6 Create a Parquet dataset in results/routes.parquet using Apache Arrow and Pandas . Do not use any compression on the output at this stage. d. Protocol Buffers \u00b6 Define a Protocol Buffers message format in schemas/routes.proto . Using the Protocol Buffers Python tutorial as a guide, compile the routes.proto into Python classes. Output the generated code into dsc650/assignment/assignment03/routes_pb2.py . Use this generated code to create results/routes.pb using Protocol Buffers. Do not use any compression on the output at this stage. e. Output Sizes \u00b6 Compare the output sizes of the different formats. Populate the results in results/comparison.csv . Assignment 3.2 \u00b6 This part of the assignment involves developing a rudimentary database index for our routes dataset. Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash . Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer. Below is an example of the geohashed value for Bellevue University. import pygeohash pygeohash . encode ( 41.1499988 , - 95.91779 ) '9z7f174u17zb' Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. The following table gives cell width and height values for each level of precision of the geohash. Geohash Coordinates Cell Width 1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m As you can see, it only takes about four levels characters to get to a 40 km by 20 km area. Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy. a. Create a Simple Geohash Index \u00b6 Using pygeohash create a simple index for the routes.jsonl.gz data using the source airport latitude and longitude. Output the index and values to the results/geoindex directory. The output looks like the following directory structure. geoindex \u251c\u2500\u2500 2 \u2502 \u251c\u2500\u2500 2e \u2502 \u2502 \u251c\u2500\u2500 2eg.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2ev.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2ey.jsonl.gz \u2502 \u251c\u2500\u2500 2h \u2502 \u2502 \u251c\u2500\u2500 2h5.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2hb.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2hx.jsonl.gz \u2502 \u251c\u2500\u2500 2j \u2502 \u2502 \u251c\u2500\u2500 2j0.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2j3.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2jd.jsonl.gz . . . \u2502 \u2514\u2500\u2500 yu \u2502 \u2514\u2500\u2500 yue.jsonl.gz \u2514\u2500\u2500 z \u251c\u2500\u2500 z0 \u2502 \u251c\u2500\u2500 z08.jsonl.gz \u2502 \u251c\u2500\u2500 z0h.jsonl.gz \u2502 \u2514\u2500\u2500 z0m.jsonl.gz \u251c\u2500\u2500 z6 \u2502 \u2514\u2500\u2500 z6e.jsonl.gz \u251c\u2500\u2500 z9 \u2502 \u2514\u2500\u2500 z92.jsonl.gz \u251c\u2500\u2500 zg \u2502 \u2514\u2500\u2500 zgw.jsonl.gz \u251c\u2500\u2500 zk \u2502 \u2514\u2500\u2500 zk9.jsonl.gz \u251c\u2500\u2500 zs \u2502 \u2514\u2500\u2500 zs4.jsonl.gz \u2514\u2500\u2500 zu \u2514\u2500\u2500 zu3.jsonl.gz b. Implement a Simple Search Feature \u00b6 Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the geohash_approximate_distance function in pygeohash to compute distances between geohash values. It returns distances in meters, but your search function should use kilometers as input. import pygeohash pygeohash . geohash_approximate_distance ( 'bcd3u' , 'bc83n' ) # >>> 625441 c. Evolve the Protocol Buffers and Avro Schemas \u00b6 Protocol Buffers, Avro, and Parquet all allow for schema evolution. Create an updated Avro schemas that include the field geohash for source and destination airports. Create this in the schemas directory as routesv2.avsc . Use this new schema to validate the previously created routes.avro dataset. If you evolved your schema correctly, the new schema should be backward compatible. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment03/ directory. Use the naming convention of assignment03_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment03_DoeJane.zip assignment03 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment03 -DestinationPath ' assignment03_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 results \u2502 \u251c\u2500\u2500 comparison.csv \u2502 \u251c\u2500\u2500 geoindex \u2502 \u251c\u2500\u2500 routes.avro \u2502 \u251c\u2500\u2500 routes.parquet \u2502 \u251c\u2500\u2500 routes.pb \u2502 \u2514\u2500\u2500 validation-results.csv \u251c\u2500\u2500 routes_pb2.py \u251c\u2500\u2500 routesv2_pb2.py \u251c\u2500\u2500 run_assignment.py \u251c\u2500\u2500 schemas \u2502 \u251c\u2500\u2500 routes-schema.json \u2502 \u251c\u2500\u2500 routes.avsc \u2502 \u251c\u2500\u2500 routes.proto \u2502 \u251c\u2500\u2500 routesv2.avsc \u2502 \u2514\u2500\u2500 routesv2.proto \u2514\u2500\u2500 util.py Your assignment may also contain additional files depending on you choose to implement your code. Discussion \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database? Topic 2 \u00b6 Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format. Topic 3 \u00b6 Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data? Topic 4 \u00b6 We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9","title":"Week 3"},{"location":"lessons/10-week/week03/#week-3","text":"In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes.","title":"Week 3"},{"location":"lessons/10-week/week03/#objectives","text":"After completing this week, you should be able to: Compare indexing algorithms including hash indexes and B-Trees Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution","title":"Objectives"},{"location":"lessons/10-week/week03/#readings","text":"Read chapters 3 and 4 in Designing Data-Intensive Applications","title":"Readings"},{"location":"lessons/10-week/week03/#weekly-resources","text":"Apache Arrow Apache Parquet Apache Thrift Apache Avro JSON Schema Protocol Buffers","title":"Weekly Resources"},{"location":"lessons/10-week/week03/#assignment-3","text":"For this assignment, you will be working with data from OpenFlights . This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in data/external/openflights/ . For this assignment, you will use a dataset derived from that original data. You can find this data in data/processed/openflights/routes.jsonl.gz . The data is compressed with gzip and encoded in the JSON Lines format . Each line represents a single airline route. The dsc650/assignments/assignment03 directory contains placeholder code and data outputs for this assignment.","title":"Assignment 3"},{"location":"lessons/10-week/week03/#assignment-31","text":"In the first part of the assignment, you will be creating schemas for the route data and encoding the routes.jsonl.gz using Protocol Buffers, Avro, and Parquet.","title":"Assignment 3.1"},{"location":"lessons/10-week/week03/#a-json-schema","text":"Create a JSON Schema in the schemas/routes-schema.json file to describe a route and validate the data in routes.jsonl.gz using the jsonschema library.","title":"a. JSON Schema"},{"location":"lessons/10-week/week03/#b-avro","text":"Create an Avro schema in the schemas/routes.avsc file to describe a route and validate the data in routes.jsonl.gz . Use Avro's Python library or fastavro library to create results/routes.avro with the schema you created. Do not use any compression on the output at this stage.","title":"b. Avro"},{"location":"lessons/10-week/week03/#c-parquet","text":"Create a Parquet dataset in results/routes.parquet using Apache Arrow and Pandas . Do not use any compression on the output at this stage.","title":"c. Parquet"},{"location":"lessons/10-week/week03/#d-protocol-buffers","text":"Define a Protocol Buffers message format in schemas/routes.proto . Using the Protocol Buffers Python tutorial as a guide, compile the routes.proto into Python classes. Output the generated code into dsc650/assignment/assignment03/routes_pb2.py . Use this generated code to create results/routes.pb using Protocol Buffers. Do not use any compression on the output at this stage.","title":"d. Protocol Buffers"},{"location":"lessons/10-week/week03/#e-output-sizes","text":"Compare the output sizes of the different formats. Populate the results in results/comparison.csv .","title":"e. Output Sizes"},{"location":"lessons/10-week/week03/#assignment-32","text":"This part of the assignment involves developing a rudimentary database index for our routes dataset. Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash . Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer. Below is an example of the geohashed value for Bellevue University. import pygeohash pygeohash . encode ( 41.1499988 , - 95.91779 ) '9z7f174u17zb' Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. The following table gives cell width and height values for each level of precision of the geohash. Geohash Coordinates Cell Width 1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m As you can see, it only takes about four levels characters to get to a 40 km by 20 km area. Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy.","title":"Assignment 3.2"},{"location":"lessons/10-week/week03/#a-create-a-simple-geohash-index","text":"Using pygeohash create a simple index for the routes.jsonl.gz data using the source airport latitude and longitude. Output the index and values to the results/geoindex directory. The output looks like the following directory structure. geoindex \u251c\u2500\u2500 2 \u2502 \u251c\u2500\u2500 2e \u2502 \u2502 \u251c\u2500\u2500 2eg.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2ev.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2ey.jsonl.gz \u2502 \u251c\u2500\u2500 2h \u2502 \u2502 \u251c\u2500\u2500 2h5.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2hb.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2hx.jsonl.gz \u2502 \u251c\u2500\u2500 2j \u2502 \u2502 \u251c\u2500\u2500 2j0.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2j3.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2jd.jsonl.gz . . . \u2502 \u2514\u2500\u2500 yu \u2502 \u2514\u2500\u2500 yue.jsonl.gz \u2514\u2500\u2500 z \u251c\u2500\u2500 z0 \u2502 \u251c\u2500\u2500 z08.jsonl.gz \u2502 \u251c\u2500\u2500 z0h.jsonl.gz \u2502 \u2514\u2500\u2500 z0m.jsonl.gz \u251c\u2500\u2500 z6 \u2502 \u2514\u2500\u2500 z6e.jsonl.gz \u251c\u2500\u2500 z9 \u2502 \u2514\u2500\u2500 z92.jsonl.gz \u251c\u2500\u2500 zg \u2502 \u2514\u2500\u2500 zgw.jsonl.gz \u251c\u2500\u2500 zk \u2502 \u2514\u2500\u2500 zk9.jsonl.gz \u251c\u2500\u2500 zs \u2502 \u2514\u2500\u2500 zs4.jsonl.gz \u2514\u2500\u2500 zu \u2514\u2500\u2500 zu3.jsonl.gz","title":"a. Create a Simple Geohash Index"},{"location":"lessons/10-week/week03/#b-implement-a-simple-search-feature","text":"Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the geohash_approximate_distance function in pygeohash to compute distances between geohash values. It returns distances in meters, but your search function should use kilometers as input. import pygeohash pygeohash . geohash_approximate_distance ( 'bcd3u' , 'bc83n' ) # >>> 625441","title":"b. Implement a Simple Search Feature"},{"location":"lessons/10-week/week03/#c-evolve-the-protocol-buffers-and-avro-schemas","text":"Protocol Buffers, Avro, and Parquet all allow for schema evolution. Create an updated Avro schemas that include the field geohash for source and destination airports. Create this in the schemas directory as routesv2.avsc . Use this new schema to validate the previously created routes.avro dataset. If you evolved your schema correctly, the new schema should be backward compatible.","title":"c.  Evolve the Protocol Buffers and Avro Schemas"},{"location":"lessons/10-week/week03/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment03/ directory. Use the naming convention of assignment03_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment03_DoeJane.zip assignment03 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment03 -DestinationPath ' assignment03_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 results \u2502 \u251c\u2500\u2500 comparison.csv \u2502 \u251c\u2500\u2500 geoindex \u2502 \u251c\u2500\u2500 routes.avro \u2502 \u251c\u2500\u2500 routes.parquet \u2502 \u251c\u2500\u2500 routes.pb \u2502 \u2514\u2500\u2500 validation-results.csv \u251c\u2500\u2500 routes_pb2.py \u251c\u2500\u2500 routesv2_pb2.py \u251c\u2500\u2500 run_assignment.py \u251c\u2500\u2500 schemas \u2502 \u251c\u2500\u2500 routes-schema.json \u2502 \u251c\u2500\u2500 routes.avsc \u2502 \u251c\u2500\u2500 routes.proto \u2502 \u251c\u2500\u2500 routesv2.avsc \u2502 \u2514\u2500\u2500 routesv2.proto \u2514\u2500\u2500 util.py Your assignment may also contain additional files depending on you choose to implement your code.","title":"Submission Instructions"},{"location":"lessons/10-week/week03/#discussion","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/10-week/week03/#topic-1","text":"Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database?","title":"Topic 1"},{"location":"lessons/10-week/week03/#topic-2","text":"Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format.","title":"Topic 2"},{"location":"lessons/10-week/week03/#topic-3","text":"Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data?","title":"Topic 3"},{"location":"lessons/10-week/week03/#topic-4","text":"We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9","title":"Topic 4"},{"location":"lessons/10-week/week04/","text":"Week 4 \u00b6 In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks. Objectives \u00b6 After completing this week, you should be able to: Implement a rudimentary version of the MapReduce paradigm in Python Create a simple deep learning network using Keras and TensorFlow Design, implement and run a big data workflow using Luigi Readings \u00b6 Read chapter 10 in Designing Data-Intensive Applications Read chapter 3 in Deep Learning with Python Weekly Resources \u00b6 Celery Architecture Kubernetes Architecture HDFS Architecture YARN Architecture Assignment 4 \u00b6 In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in email package to read the emails and create a machine-friendly dataset. The data/external/enron folder contains a partial copy of the original Enron email dataset (you can download the full dataset here ). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. enron/zipper-a \u251c\u2500\u2500 all_documents \u2502 \u251c\u2500\u2500 1 . \u2502 \u251c\u2500\u2500 10 . \u2502 \u251c\u2500\u2500 11 . \u2502 \u251c\u2500\u2500 12 . \u2502 \u251c\u2500\u2500 13 . \u2502 \u251c\u2500\u2500 14 . . . . \u2502 \u251c\u2500\u2500 8 . \u2502 \u2514\u2500\u2500 9 . \u2514\u2500\u2500 tss \u251c\u2500\u2500 1 . \u251c\u2500\u2500 10 . \u251c\u2500\u2500 11 . \u251c\u2500\u2500 12 . . . . \u251c\u2500\u2500 4 . \u251c\u2500\u2500 5 . \u251c\u2500\u2500 6 . \u251c\u2500\u2500 7 . \u251c\u2500\u2500 8 . \u2514\u2500\u2500 9 . Looking at the example of /enron/zipper-a/inbox/114. demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body. This is typical of most of the emails except some email bodies being encoded in HTML. Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header The dsc650/assignments/assignment04 folder contains partially completed code and placeholder files for this assignment. Assignment 4.1 \u00b6 The first part of the assignment is to implement a single function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. The folder dsc650/assignments/assignment04/examples contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your read_email function is working properly. Assignment 4.2 \u00b6 Next, you will be creating a workflow using the Luigi Python library. This assignment uses Luigi because it is a self-contained Python package and does not require any additional configuration to run. There are many other workflow managers including Apache Airflow , Apache Oozie , LinkedIn's Azkaban , Netflix's Conductor , and Argo for Kubernetes . Luigi, like most workflow engines, breaks workflows into discrete tasks. Individual tasks chain together into workflows by telling the workflow engine which tasks depend on one another. These task dependencies take the form of a directed acyclic graph (DAG). While this may sound complicated, a DAG is a flowchart of which tasks should be executed in what order (the order means it is directed) with the constraint that later tasks cannot loop back and depend on earlier tasks (hence the acyclic requirement). Read the Luigi documentation on its execution model for more information. To start with, you will have one wrapper task that triggers a task to process each folder. Later, we will add tasks that process the outputs of those tasks. The following code provides a rough outline of this workflow. import luigi class ProcessMailbox ( luigi . Task ): mailbox_directory = luigi . Parameter () processed_directory = luigi . Parameter () def output ( self ): pass def run ( self ): pass class ProcessEnronEmails ( luigi . WrapperTask ): emails_directory = luigi . Parameter () processed_directory = luigi . Parameter () def requires ( self ): for directory in directories : yield ProcessMailbox ( mailbox_directory = str ( directory ), processed_directory = str ( self . processed_directory ) ) def main (): tasks = [ ProcessEnronEmails ( emails_directory = emails_directory , processed_directory = processed_directory ) ] luigi . build ( tasks , workers = 8 , local_scheduler = True ) if __name__ == '__main__' : main () Create and run this workflow. The following is the example of a workflow that completed with one failed task. Failures could be caused by problems with the workflow or problems with the data. The advantage of using a tool like Luigi is that you don't need to re-run all the tasks, only the ones that failed. ===== Luigi Execution Summary ===== Scheduled 151 tasks of which: * 23 complete ones were encountered: - 23 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/dean-c, processed_directory = dsc650/data/processed/enron ) ... * 126 ran successfully: - 126 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/allen-p, processed_directory = dsc650/data/processed/enron ) ... * 1 failed: - 1 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/stokley-c, processed_directory = dsc650/data/processed/enron ) * 1 were left pending, among these: * 1 had failed dependencies: - 1 ProcessEnronEmails ( emails_directory = dsc650/data/external/enron, processed_directory = dsc650/data/processed/enron ) This progress looks : ( because there were failed tasks ===== Luigi Execution Summary ===== This is an example of a workflow that ran without errors. INFO: ===== Luigi Execution Summary ===== Scheduled 12 tasks of which: * 12 ran successfully: - 1 ProcessEnronEmails ( emails_directory = dsc650/data/external/enron, processed_directory = dsc650/data/processed/enron ) - 11 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/davis-d, processed_directory = dsc650/data/processed/enron ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Additionally, Luigi has a web-based central scheduler that you use to view and manage the progress of your workflows. Assignment 4.3 \u00b6 Now that you have processed the emails and extracted the text payload, you are going to further process them with a simple MapReduce program. We will start by performing a simple word count on text data; the hello world of MapReduce. The Spark Examples page has a simple example of a word count in Python. We will walk through this example together and then you will implement your version of MapReduce in Python. text_file = sc . textFile ( \"hdfs://...\" ) counts = text_file . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word : ( word , 1 )) \\ . reduceByKey ( lambda a , b : a + b ) counts . saveAsTextFile ( \"hdfs://...\" ) While the example may look complicated, the entire program consists of applying three functions to a text file. After the program loads the text file and assigns it to the text_file variable, it applies a function to each line in the text file. And outputs a list of words. The program uses the flatMap because it produces multiple outputs for every input parameter (i.e., multiple words for every line). The map function then takes each of those words and outputs a key-value pair for each word. If we had the sentence The quick brown fox jumps over the lazy dog , the map function would output the following key-value pairs. def split_to_words(line): for word in line.split(\" \") yield (word, 1) split_to_words('The quick brown fox jumps over the lazy dog') (The, 1) (quick, 1) (brown, 1) (fox, 1) (jumps, 1) (over, 1) (the, 1) (lazy, 1) (dog, 1) These steps are part of the map stage of the MapReduce programming paradigm. Map functions scale extremely well to large datasets as they can be run in parallel without any coordination. Each mapper takes a different chunk of the data and outputs a series of key-value pairs. By contrast, the reduce step requires sorting the key-value pairs and combine the values with common keys. To illustrate, suppose we have two mappers with the following output from the first mapper. (the, 1) (data, 1) (the, 1) (data, 1) (jump, 1) (data, 1) Then suppose the second mapper has this output. (the, 1) (hide, 1) (data, 1) (the, 1) (data, 1) The reduce phase sorts each of the mapper outputs by their keys and combines. Thus, the reducer would act on the outputs as follow. reducer( (the, 1), (the, 1), (the, 1), (the, 1) ) -> (the, 4) reducer( (data, 1), (data, 1), (data, 1), (data, 1), (data, 1) ) -> (data, 5) reducer((jump, 1)) -> (jump, 1) reducer((hide, 1)) -> (hide, 1) After combining all the results of all the mappers with the reducers, we then have a count for each of the words. Implement a mapper as a Luigi task that outputs the words and counts for each of the mailboxes. Then implement a reducer that aggregates all of those counts into a combined count. The dsc650/assignments/assignment04/results/ should look like as follows. results \u251c\u2500\u2500 count.txt \u2514\u2500\u2500 words \u251c\u2500\u2500 davis-d.txt \u251c\u2500\u2500 gay-r.txt \u251c\u2500\u2500 may-l.txt . . . \u2514\u2500\u2500 zipper-a.txt These tasks should be a part of the previous workflow you created. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment04/ directory. Use the naming convention of assignment04_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment04_DoeJane.zip assignment04 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment04 -DestinationPath ' assignment04_DoeJane.zip Discussion \u00b6 For this discussion, describe a batch workflow use case that you would run on a daily, weekly, or monthly basis. What are the inputs and the outputs? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 4"},{"location":"lessons/10-week/week04/#week-4","text":"In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks.","title":"Week 4"},{"location":"lessons/10-week/week04/#objectives","text":"After completing this week, you should be able to: Implement a rudimentary version of the MapReduce paradigm in Python Create a simple deep learning network using Keras and TensorFlow Design, implement and run a big data workflow using Luigi","title":"Objectives"},{"location":"lessons/10-week/week04/#readings","text":"Read chapter 10 in Designing Data-Intensive Applications Read chapter 3 in Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week04/#weekly-resources","text":"Celery Architecture Kubernetes Architecture HDFS Architecture YARN Architecture","title":"Weekly Resources"},{"location":"lessons/10-week/week04/#assignment-4","text":"In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in email package to read the emails and create a machine-friendly dataset. The data/external/enron folder contains a partial copy of the original Enron email dataset (you can download the full dataset here ). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. enron/zipper-a \u251c\u2500\u2500 all_documents \u2502 \u251c\u2500\u2500 1 . \u2502 \u251c\u2500\u2500 10 . \u2502 \u251c\u2500\u2500 11 . \u2502 \u251c\u2500\u2500 12 . \u2502 \u251c\u2500\u2500 13 . \u2502 \u251c\u2500\u2500 14 . . . . \u2502 \u251c\u2500\u2500 8 . \u2502 \u2514\u2500\u2500 9 . \u2514\u2500\u2500 tss \u251c\u2500\u2500 1 . \u251c\u2500\u2500 10 . \u251c\u2500\u2500 11 . \u251c\u2500\u2500 12 . . . . \u251c\u2500\u2500 4 . \u251c\u2500\u2500 5 . \u251c\u2500\u2500 6 . \u251c\u2500\u2500 7 . \u251c\u2500\u2500 8 . \u2514\u2500\u2500 9 . Looking at the example of /enron/zipper-a/inbox/114. demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body. This is typical of most of the emails except some email bodies being encoded in HTML. Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header The dsc650/assignments/assignment04 folder contains partially completed code and placeholder files for this assignment.","title":"Assignment 4"},{"location":"lessons/10-week/week04/#assignment-41","text":"The first part of the assignment is to implement a single function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. The folder dsc650/assignments/assignment04/examples contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your read_email function is working properly.","title":"Assignment 4.1"},{"location":"lessons/10-week/week04/#assignment-42","text":"Next, you will be creating a workflow using the Luigi Python library. This assignment uses Luigi because it is a self-contained Python package and does not require any additional configuration to run. There are many other workflow managers including Apache Airflow , Apache Oozie , LinkedIn's Azkaban , Netflix's Conductor , and Argo for Kubernetes . Luigi, like most workflow engines, breaks workflows into discrete tasks. Individual tasks chain together into workflows by telling the workflow engine which tasks depend on one another. These task dependencies take the form of a directed acyclic graph (DAG). While this may sound complicated, a DAG is a flowchart of which tasks should be executed in what order (the order means it is directed) with the constraint that later tasks cannot loop back and depend on earlier tasks (hence the acyclic requirement). Read the Luigi documentation on its execution model for more information. To start with, you will have one wrapper task that triggers a task to process each folder. Later, we will add tasks that process the outputs of those tasks. The following code provides a rough outline of this workflow. import luigi class ProcessMailbox ( luigi . Task ): mailbox_directory = luigi . Parameter () processed_directory = luigi . Parameter () def output ( self ): pass def run ( self ): pass class ProcessEnronEmails ( luigi . WrapperTask ): emails_directory = luigi . Parameter () processed_directory = luigi . Parameter () def requires ( self ): for directory in directories : yield ProcessMailbox ( mailbox_directory = str ( directory ), processed_directory = str ( self . processed_directory ) ) def main (): tasks = [ ProcessEnronEmails ( emails_directory = emails_directory , processed_directory = processed_directory ) ] luigi . build ( tasks , workers = 8 , local_scheduler = True ) if __name__ == '__main__' : main () Create and run this workflow. The following is the example of a workflow that completed with one failed task. Failures could be caused by problems with the workflow or problems with the data. The advantage of using a tool like Luigi is that you don't need to re-run all the tasks, only the ones that failed. ===== Luigi Execution Summary ===== Scheduled 151 tasks of which: * 23 complete ones were encountered: - 23 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/dean-c, processed_directory = dsc650/data/processed/enron ) ... * 126 ran successfully: - 126 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/allen-p, processed_directory = dsc650/data/processed/enron ) ... * 1 failed: - 1 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/stokley-c, processed_directory = dsc650/data/processed/enron ) * 1 were left pending, among these: * 1 had failed dependencies: - 1 ProcessEnronEmails ( emails_directory = dsc650/data/external/enron, processed_directory = dsc650/data/processed/enron ) This progress looks : ( because there were failed tasks ===== Luigi Execution Summary ===== This is an example of a workflow that ran without errors. INFO: ===== Luigi Execution Summary ===== Scheduled 12 tasks of which: * 12 ran successfully: - 1 ProcessEnronEmails ( emails_directory = dsc650/data/external/enron, processed_directory = dsc650/data/processed/enron ) - 11 ProcessMailbox ( mailbox_directory = dsc650/data/external/enron/davis-d, processed_directory = dsc650/data/processed/enron ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Additionally, Luigi has a web-based central scheduler that you use to view and manage the progress of your workflows.","title":"Assignment 4.2"},{"location":"lessons/10-week/week04/#assignment-43","text":"Now that you have processed the emails and extracted the text payload, you are going to further process them with a simple MapReduce program. We will start by performing a simple word count on text data; the hello world of MapReduce. The Spark Examples page has a simple example of a word count in Python. We will walk through this example together and then you will implement your version of MapReduce in Python. text_file = sc . textFile ( \"hdfs://...\" ) counts = text_file . flatMap ( lambda line : line . split ( \" \" )) \\ . map ( lambda word : ( word , 1 )) \\ . reduceByKey ( lambda a , b : a + b ) counts . saveAsTextFile ( \"hdfs://...\" ) While the example may look complicated, the entire program consists of applying three functions to a text file. After the program loads the text file and assigns it to the text_file variable, it applies a function to each line in the text file. And outputs a list of words. The program uses the flatMap because it produces multiple outputs for every input parameter (i.e., multiple words for every line). The map function then takes each of those words and outputs a key-value pair for each word. If we had the sentence The quick brown fox jumps over the lazy dog , the map function would output the following key-value pairs. def split_to_words(line): for word in line.split(\" \") yield (word, 1) split_to_words('The quick brown fox jumps over the lazy dog') (The, 1) (quick, 1) (brown, 1) (fox, 1) (jumps, 1) (over, 1) (the, 1) (lazy, 1) (dog, 1) These steps are part of the map stage of the MapReduce programming paradigm. Map functions scale extremely well to large datasets as they can be run in parallel without any coordination. Each mapper takes a different chunk of the data and outputs a series of key-value pairs. By contrast, the reduce step requires sorting the key-value pairs and combine the values with common keys. To illustrate, suppose we have two mappers with the following output from the first mapper. (the, 1) (data, 1) (the, 1) (data, 1) (jump, 1) (data, 1) Then suppose the second mapper has this output. (the, 1) (hide, 1) (data, 1) (the, 1) (data, 1) The reduce phase sorts each of the mapper outputs by their keys and combines. Thus, the reducer would act on the outputs as follow. reducer( (the, 1), (the, 1), (the, 1), (the, 1) ) -> (the, 4) reducer( (data, 1), (data, 1), (data, 1), (data, 1), (data, 1) ) -> (data, 5) reducer((jump, 1)) -> (jump, 1) reducer((hide, 1)) -> (hide, 1) After combining all the results of all the mappers with the reducers, we then have a count for each of the words. Implement a mapper as a Luigi task that outputs the words and counts for each of the mailboxes. Then implement a reducer that aggregates all of those counts into a combined count. The dsc650/assignments/assignment04/results/ should look like as follows. results \u251c\u2500\u2500 count.txt \u2514\u2500\u2500 words \u251c\u2500\u2500 davis-d.txt \u251c\u2500\u2500 gay-r.txt \u251c\u2500\u2500 may-l.txt . . . \u2514\u2500\u2500 zipper-a.txt These tasks should be a part of the previous workflow you created.","title":"Assignment 4.3"},{"location":"lessons/10-week/week04/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment04/ directory. Use the naming convention of assignment04_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment04_DoeJane.zip assignment04 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment04 -DestinationPath ' assignment04_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week04/#discussion","text":"For this discussion, describe a batch workflow use case that you would run on a daily, weekly, or monthly basis. What are the inputs and the outputs? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/10-week/week05/","text":"Week 5 \u00b6 In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python . This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects. Objectives \u00b6 After completing this week, you should be able to: Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression Create workflows that train deep learning models and then produce validation and metrics on those models Readings \u00b6 Read chapters 3 and 4 Deep Learning with Python Weekly Resources \u00b6 Assignment 5 \u00b6 In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python . You will use that code to create a Luigi pipeline that trains the model, uses the model to perform model validation, and output model metrics. Assignment 5.1 \u00b6 Implement the movie review classifier found in section 3.4 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ . Assignment 5.2 \u00b6 Implement the news classifier found in section 3.5 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ . Assignment 5.3 \u00b6 Implement the housing price regression model found in section 3.6 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment05/ directory. Use the naming convention of assignment05_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment05_DoeJane.zip assignment05 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment05 -DestinationPath ' assignment05_DoeJane.zip Discussion Board \u00b6 For this discussion, write a 250 to 750-word discussion board post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests. In this use case, how often would you need to train the models? How would you deploy the models? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 5"},{"location":"lessons/10-week/week05/#week-5","text":"In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python . This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects.","title":"Week 5"},{"location":"lessons/10-week/week05/#objectives","text":"After completing this week, you should be able to: Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression Create workflows that train deep learning models and then produce validation and metrics on those models","title":"Objectives"},{"location":"lessons/10-week/week05/#readings","text":"Read chapters 3 and 4 Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week05/#weekly-resources","text":"","title":"Weekly Resources"},{"location":"lessons/10-week/week05/#assignment-5","text":"In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python . You will use that code to create a Luigi pipeline that trains the model, uses the model to perform model validation, and output model metrics.","title":"Assignment 5"},{"location":"lessons/10-week/week05/#assignment-51","text":"Implement the movie review classifier found in section 3.4 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ .","title":"Assignment 5.1"},{"location":"lessons/10-week/week05/#assignment-52","text":"Implement the news classifier found in section 3.5 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ .","title":"Assignment 5.2"},{"location":"lessons/10-week/week05/#assignment-53","text":"Implement the housing price regression model found in section 3.6 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in dsc650/assignments/assignment05/ .","title":"Assignment 5.3"},{"location":"lessons/10-week/week05/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment05/ directory. Use the naming convention of assignment05_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment05_DoeJane.zip assignment05 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment05 -DestinationPath ' assignment05_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week05/#discussion-board","text":"For this discussion, write a 250 to 750-word discussion board post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests. In this use case, how often would you need to train the models? How would you deploy the models? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/10-week/week06/","text":"Week 6 \u00b6 In this lesson you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection. Objectives \u00b6 After completing this week, you should be able to: Build a ConvNet from labeled image data to perform multiple category image classification Understand how to use existing models to classify images Describe how to fine-tune existing models for specific classification tasks Readings \u00b6 Read chapter 5 in Deep Learning with Python Weekly Resources \u00b6 CIFAR-10 DataSet Common Objects in Context COCO COCO Dataset TensorFlow Image Captioning TensorFlow Transfer Learning You Only Look Once: Unified, Real-Time Object Detection Assignment 6 \u00b6 Assignment 6.1 \u00b6 Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. Assignment 6.2 \u00b6 a. \u00b6 Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. b. \u00b6 Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. Assignment 6.3 \u00b6 Load the ResNet50 model and classify the images found in the data/raw/images directory. Save the predictions dsc650/assignments/assignment06/results/predictions/resnet50 directory. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment06/ directory. Use the naming convention of assignment06_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment06_DoeJane.zip assignment06 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment06 -DestinationPath ' assignment06_DoeJane.zip Discussion Board \u00b6 In this lesson, we focused on using ConvNets to classify entire images. In real-world use cases, we often want to perform different tasks such as object detection, image captioning, or face detection. For this discussion, pick one of the three topics below and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 - Transfer Learning \u00b6 Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model. How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch? Topic 2 - Object detection \u00b6 In this lesson you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image. What use cases do you see for object detection? What techniques would you use to perform object detection? Topic 3 - Face Detection \u00b6 Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data?","title":"Week 6"},{"location":"lessons/10-week/week06/#week-6","text":"In this lesson you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection.","title":"Week 6"},{"location":"lessons/10-week/week06/#objectives","text":"After completing this week, you should be able to: Build a ConvNet from labeled image data to perform multiple category image classification Understand how to use existing models to classify images Describe how to fine-tune existing models for specific classification tasks","title":"Objectives"},{"location":"lessons/10-week/week06/#readings","text":"Read chapter 5 in Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week06/#weekly-resources","text":"CIFAR-10 DataSet Common Objects in Context COCO COCO Dataset TensorFlow Image Captioning TensorFlow Transfer Learning You Only Look Once: Unified, Real-Time Object Detection","title":"Weekly Resources"},{"location":"lessons/10-week/week06/#assignment-6","text":"","title":"Assignment 6"},{"location":"lessons/10-week/week06/#assignment-61","text":"Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory.","title":"Assignment 6.1"},{"location":"lessons/10-week/week06/#assignment-62","text":"","title":"Assignment 6.2"},{"location":"lessons/10-week/week06/#a","text":"Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory.","title":"a."},{"location":"lessons/10-week/week06/#b","text":"Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory.","title":"b."},{"location":"lessons/10-week/week06/#assignment-63","text":"Load the ResNet50 model and classify the images found in the data/raw/images directory. Save the predictions dsc650/assignments/assignment06/results/predictions/resnet50 directory.","title":"Assignment 6.3"},{"location":"lessons/10-week/week06/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment06/ directory. Use the naming convention of assignment06_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment06_DoeJane.zip assignment06 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment06 -DestinationPath ' assignment06_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week06/#discussion-board","text":"In this lesson, we focused on using ConvNets to classify entire images. In real-world use cases, we often want to perform different tasks such as object detection, image captioning, or face detection. For this discussion, pick one of the three topics below and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/10-week/week06/#topic-1-transfer-learning","text":"Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model. How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch?","title":"Topic 1 - Transfer Learning"},{"location":"lessons/10-week/week06/#topic-2-object-detection","text":"In this lesson you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image. What use cases do you see for object detection? What techniques would you use to perform object detection?","title":"Topic 2 - Object detection"},{"location":"lessons/10-week/week06/#topic-3-face-detection","text":"Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data?","title":"Topic 3 - Face Detection"},{"location":"lessons/10-week/week07/","text":"Week 7 \u00b6 In previous lessons, we covered how to encode data in different formats and the basics of different query languages. Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data. Objectives \u00b6 After completing this week, you should be able to: Compare and contrast different data replication strategies and discuss their advantages and disadvantages Implement basic data partitioning paradigms using Python and Parquet Describe how partitioning and replication affects data queries Readings \u00b6 Read chapters 5 and 6 in Designing Data-Intensive Applications Weekly Resources \u00b6 Cassandra HDFS Architecture YARN Architecture Assignment 7 \u00b6 Assignment 7.1 \u00b6 In this part of the assignment, you will partition a dataset using different strategies. You will use the routes.parquet dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline. For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of OMADENAA . a. \u00b6 Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called key . For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment. The partitions are determined by the first letter of the composite key using the following partitions. partitions = ( ( 'A' , 'A' ), ( 'B' , 'B' ), ( 'C' , 'D' ), ( 'E' , 'F' ), ( 'G' , 'H' ), ( 'I' , 'J' ), ( 'K' , 'L' ), ( 'M' , 'M' ), ( 'N' , 'N' ), ( 'O' , 'P' ), ( 'Q' , 'R' ), ( 'S' , 'T' ), ( 'U' , 'U' ), ( 'V' , 'V' ), ( 'W' , 'X' ), ( 'Y' , 'Z' ) ) In this case ('A', 'A') means the folder should contain all of the routes whose composite key starts with A . Similarly, ('E', 'F') should contain routes whose composite key starts with E or F . The results/kv directory should contain the following folders. kv \u251c\u2500\u2500 kv_key = A \u251c\u2500\u2500 kv_key = B \u251c\u2500\u2500 kv_key = C-D \u251c\u2500\u2500 kv_key = E-F \u251c\u2500\u2500 kv_key = G-H \u251c\u2500\u2500 kv_key = I-J \u251c\u2500\u2500 kv_key = K-L \u251c\u2500\u2500 kv_key = M \u251c\u2500\u2500 kv_key = N \u251c\u2500\u2500 kv_key = O-P \u251c\u2500\u2500 kv_key = Q-R \u251c\u2500\u2500 kv_key = S-T \u251c\u2500\u2500 kv_key = U \u251c\u2500\u2500 kv_key = V \u251c\u2500\u2500 kv_key = W-X \u2514\u2500\u2500 kv_key = Y-Z An easy way to create this directory structure is to create a new key called kv_key from the key column and use the to_parquet method with partition_cols=['kv_key'] to save a partitioned dataset. b. \u00b6 Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key. The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. import hashlib def hash_key ( key ): m = hashlib . sha256 () m . update ( str ( key ) . encode ( 'utf-8' )) return m . hexdigest () We will partition the data using the first character of the hexadecimal hash. As such, there are 16 possible partitions. Create a new column called hashed that is a hashed value of the key column. Next, create a partitioned dataset based on the first character of the hashed key and save the results to results/hash . The directory should contain the following folders. hash \u251c\u2500\u2500 hash_key = 0 \u251c\u2500\u2500 hash_key = 1 \u251c\u2500\u2500 hash_key = 2 \u251c\u2500\u2500 hash_key = 3 \u251c\u2500\u2500 hash_key = 4 \u251c\u2500\u2500 hash_key = 5 \u251c\u2500\u2500 hash_key = 6 \u251c\u2500\u2500 hash_key = 7 \u251c\u2500\u2500 hash_key = 8 \u251c\u2500\u2500 hash_key = 9 \u251c\u2500\u2500 hash_key = A \u251c\u2500\u2500 hash_key = B \u251c\u2500\u2500 hash_key = C \u251c\u2500\u2500 hash_key = D \u251c\u2500\u2500 hash_key = E c. \u00b6 Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States. Google lists the locations of their data centers and we will use the following locations for our three data centers. * West * The Dalles, Oregon * Latitude: 45.5945645 * Longitude: -121.1786823 * Central * Papillion, NE * Latitude: 41.1544433 * Longitude: -96.0422378 * East * Loudoun County, Virginia * Latitude: 39.08344 * Longitude: -77.6497145 Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport. The output folders should look as follows. geo \u251c\u2500\u2500 location = central \u251c\u2500\u2500 location = east \u2514\u2500\u2500 location = west d. \u00b6 Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition. In other words, the partitions should be ordered. def balance_partitions ( keys , num_partitions ): partitions = [] return partitions Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment07/ directory. Use the naming convention of assignment07_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment07_DoeJane.zip assignment07 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment07 -DestinationPath ' assignment07_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Compare and contrast the different replication and partitioning strategies used by different databases. Examples include HBase, Cassandra, PostgreSQL, and DynamoDB. What are the advantages and disadvantages associated with each strategy? What use cases are best suited for each paradigm? Topic 2 \u00b6 Apache Zookeeper is a key component of many big data applications. Provide examples of Zookeeper use cases. How does Zookeeper compare to etcd ? Topic 3 \u00b6 Provide a specific example of how HBase uses key-range partitioning to speed up data queries. Describe a typical query pattern for HBase.","title":"Week 7"},{"location":"lessons/10-week/week07/#week-7","text":"In previous lessons, we covered how to encode data in different formats and the basics of different query languages. Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data.","title":"Week 7"},{"location":"lessons/10-week/week07/#objectives","text":"After completing this week, you should be able to: Compare and contrast different data replication strategies and discuss their advantages and disadvantages Implement basic data partitioning paradigms using Python and Parquet Describe how partitioning and replication affects data queries","title":"Objectives"},{"location":"lessons/10-week/week07/#readings","text":"Read chapters 5 and 6 in Designing Data-Intensive Applications","title":"Readings"},{"location":"lessons/10-week/week07/#weekly-resources","text":"Cassandra HDFS Architecture YARN Architecture","title":"Weekly Resources"},{"location":"lessons/10-week/week07/#assignment-7","text":"","title":"Assignment 7"},{"location":"lessons/10-week/week07/#assignment-71","text":"In this part of the assignment, you will partition a dataset using different strategies. You will use the routes.parquet dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline. For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of OMADENAA .","title":"Assignment 7.1"},{"location":"lessons/10-week/week07/#a","text":"Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called key . For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment. The partitions are determined by the first letter of the composite key using the following partitions. partitions = ( ( 'A' , 'A' ), ( 'B' , 'B' ), ( 'C' , 'D' ), ( 'E' , 'F' ), ( 'G' , 'H' ), ( 'I' , 'J' ), ( 'K' , 'L' ), ( 'M' , 'M' ), ( 'N' , 'N' ), ( 'O' , 'P' ), ( 'Q' , 'R' ), ( 'S' , 'T' ), ( 'U' , 'U' ), ( 'V' , 'V' ), ( 'W' , 'X' ), ( 'Y' , 'Z' ) ) In this case ('A', 'A') means the folder should contain all of the routes whose composite key starts with A . Similarly, ('E', 'F') should contain routes whose composite key starts with E or F . The results/kv directory should contain the following folders. kv \u251c\u2500\u2500 kv_key = A \u251c\u2500\u2500 kv_key = B \u251c\u2500\u2500 kv_key = C-D \u251c\u2500\u2500 kv_key = E-F \u251c\u2500\u2500 kv_key = G-H \u251c\u2500\u2500 kv_key = I-J \u251c\u2500\u2500 kv_key = K-L \u251c\u2500\u2500 kv_key = M \u251c\u2500\u2500 kv_key = N \u251c\u2500\u2500 kv_key = O-P \u251c\u2500\u2500 kv_key = Q-R \u251c\u2500\u2500 kv_key = S-T \u251c\u2500\u2500 kv_key = U \u251c\u2500\u2500 kv_key = V \u251c\u2500\u2500 kv_key = W-X \u2514\u2500\u2500 kv_key = Y-Z An easy way to create this directory structure is to create a new key called kv_key from the key column and use the to_parquet method with partition_cols=['kv_key'] to save a partitioned dataset.","title":"a."},{"location":"lessons/10-week/week07/#b","text":"Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key. The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. import hashlib def hash_key ( key ): m = hashlib . sha256 () m . update ( str ( key ) . encode ( 'utf-8' )) return m . hexdigest () We will partition the data using the first character of the hexadecimal hash. As such, there are 16 possible partitions. Create a new column called hashed that is a hashed value of the key column. Next, create a partitioned dataset based on the first character of the hashed key and save the results to results/hash . The directory should contain the following folders. hash \u251c\u2500\u2500 hash_key = 0 \u251c\u2500\u2500 hash_key = 1 \u251c\u2500\u2500 hash_key = 2 \u251c\u2500\u2500 hash_key = 3 \u251c\u2500\u2500 hash_key = 4 \u251c\u2500\u2500 hash_key = 5 \u251c\u2500\u2500 hash_key = 6 \u251c\u2500\u2500 hash_key = 7 \u251c\u2500\u2500 hash_key = 8 \u251c\u2500\u2500 hash_key = 9 \u251c\u2500\u2500 hash_key = A \u251c\u2500\u2500 hash_key = B \u251c\u2500\u2500 hash_key = C \u251c\u2500\u2500 hash_key = D \u251c\u2500\u2500 hash_key = E","title":"b."},{"location":"lessons/10-week/week07/#c","text":"Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States. Google lists the locations of their data centers and we will use the following locations for our three data centers. * West * The Dalles, Oregon * Latitude: 45.5945645 * Longitude: -121.1786823 * Central * Papillion, NE * Latitude: 41.1544433 * Longitude: -96.0422378 * East * Loudoun County, Virginia * Latitude: 39.08344 * Longitude: -77.6497145 Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport. The output folders should look as follows. geo \u251c\u2500\u2500 location = central \u251c\u2500\u2500 location = east \u2514\u2500\u2500 location = west","title":"c."},{"location":"lessons/10-week/week07/#d","text":"Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition. In other words, the partitions should be ordered. def balance_partitions ( keys , num_partitions ): partitions = [] return partitions","title":"d."},{"location":"lessons/10-week/week07/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment07/ directory. Use the naming convention of assignment07_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment07_DoeJane.zip assignment07 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment07 -DestinationPath ' assignment07_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week07/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/10-week/week07/#topic-1","text":"Compare and contrast the different replication and partitioning strategies used by different databases. Examples include HBase, Cassandra, PostgreSQL, and DynamoDB. What are the advantages and disadvantages associated with each strategy? What use cases are best suited for each paradigm?","title":"Topic 1"},{"location":"lessons/10-week/week07/#topic-2","text":"Apache Zookeeper is a key component of many big data applications. Provide examples of Zookeeper use cases. How does Zookeeper compare to etcd ?","title":"Topic 2"},{"location":"lessons/10-week/week07/#topic-3","text":"Provide a specific example of how HBase uses key-range partitioning to speed up data queries. Describe a typical query pattern for HBase.","title":"Topic 3"},{"location":"lessons/10-week/week08/","text":"In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. Objectives \u00b6 After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs Readings \u00b6 Read chapters 7, 9 and 11 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications Read Kafka Use Cases Read The Log: What every software engineer should know about real-time data's unifying abstraction Weekly Resources \u00b6 etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer REST REST Spark Structured Streaming Zookeeper Assignment 8 \u00b6 For this assignment, we will be using data from the Berkeley Deep Drive . We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming . The data/processed/bdd/ folder contains the processed data for this assignment. The accelerations folder contains accelerometer data collected from each car and the locations contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. bdd \u251c\u2500\u2500 accelerations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u2502 \u251c\u2500\u2500 1 . . . \u2514\u2500\u2500 locations \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet . . . \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u251c\u2500\u2500 t = 128 .0 \u2502 \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet \u2502 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u2502 \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u2514\u2500\u2500 t = 128 .8 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet In this example, the folder t=000.0 is the start of the simulated data. The folder t=052.2 is 52.2 seconds into the simulation and t=128.8 is 128.8 seconds into the simulation. Assignment 8.1 \u00b6 The first part of the assignment involves creating a script, dsc650/assignments/assignment08/stream_data.py , that mimics a real-time streaming data feed. The following is the directory structure of the results directory for this assignment. assignment08/results \u2514\u2500\u2500 stream \u251c\u2500\u2500 input \u251c\u2500\u2500 output \u2514\u2500\u2500 staging The basic loop for the stream_data.py script is simple. The script should load each of the processed directories in the appropriate time order. For example, once your script has passed the 52.5-second mark it should perform the following steps. Load the data from the t=052.5 directory. Calculate a new timestamp column value by adding the offset column to the datetime value of when you started the simulation. Write the updated parquet files to the results/stream/staging directory. Move the files from the staging directory to the input directory. This is necessary to prevent Spark from reading partially written files. Your results folder should look similar to the one below right before you move the data from staging to input . results \u2514\u2500\u2500 stream \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 accelerations \u2502 \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 t = 003 .2 . . . \u2502 \u2502 \u251c\u2500\u2500 t = 050 .6 \u2502 \u2502 \u2514\u2500\u2500 t = 051 .6 \u2502 \u2514\u2500\u2500 locations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 t = 004 .5 . . . \u2502 \u251c\u2500\u2500 t = 049 .5 \u2502 \u251c\u2500\u2500 t = 050 .6 \u2502 \u2514\u2500\u2500 t = 051 .6 \u251c\u2500\u2500 output \u2514\u2500\u2500 staging \u251c\u2500\u2500 accelerations \u2502 \u2514\u2500\u2500 t = 052 .5 \u2514\u2500\u2500 locations \u2514\u2500\u2500 t = 052 .5 When your script starts, you will probably want to remove any existing files in the staging and input directories. Assignment 8.2 \u00b6 In the second part of the exercise, you will create two streaming dataframes using the accelerations and locations folders. a. \u00b6 Start by creating a simple Spark Streaming application that reads data from the accelerations and locations folders in results/input and uses file sink to save the results to results/output/simple . b. \u00b6 Define a watermark on both dataframes using the timestamp column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each route. Save the results in results/output/windowed/ and set the output mode to update . c. \u00b6 Join the two streams together on the UUID as an outer join. Save the results in results/output/stream-joined . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment08/ directory. Use the naming convention of assignment08_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment08_DoeJane.zip assignment08 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment08 -DestinationPath ' assignment08_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems. What other data systems make use of this data structure? Topic 2 \u00b6 Representational State Transfer REST REST is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications? Topic 3 \u00b6 Describe how different database systems handle transactions. Pick three or more different systems to compare and contrast.","title":"Week 8"},{"location":"lessons/10-week/week08/#objectives","text":"After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs","title":"Objectives"},{"location":"lessons/10-week/week08/#readings","text":"Read chapters 7, 9 and 11 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications Read Kafka Use Cases Read The Log: What every software engineer should know about real-time data's unifying abstraction","title":"Readings"},{"location":"lessons/10-week/week08/#weekly-resources","text":"etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer REST REST Spark Structured Streaming Zookeeper","title":"Weekly Resources"},{"location":"lessons/10-week/week08/#assignment-8","text":"For this assignment, we will be using data from the Berkeley Deep Drive . We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming . The data/processed/bdd/ folder contains the processed data for this assignment. The accelerations folder contains accelerometer data collected from each car and the locations contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. bdd \u251c\u2500\u2500 accelerations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u2502 \u251c\u2500\u2500 1 . . . \u2514\u2500\u2500 locations \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet . . . \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u251c\u2500\u2500 t = 128 .0 \u2502 \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet \u2502 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u2502 \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u2514\u2500\u2500 t = 128 .8 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet In this example, the folder t=000.0 is the start of the simulated data. The folder t=052.2 is 52.2 seconds into the simulation and t=128.8 is 128.8 seconds into the simulation.","title":"Assignment 8"},{"location":"lessons/10-week/week08/#assignment-81","text":"The first part of the assignment involves creating a script, dsc650/assignments/assignment08/stream_data.py , that mimics a real-time streaming data feed. The following is the directory structure of the results directory for this assignment. assignment08/results \u2514\u2500\u2500 stream \u251c\u2500\u2500 input \u251c\u2500\u2500 output \u2514\u2500\u2500 staging The basic loop for the stream_data.py script is simple. The script should load each of the processed directories in the appropriate time order. For example, once your script has passed the 52.5-second mark it should perform the following steps. Load the data from the t=052.5 directory. Calculate a new timestamp column value by adding the offset column to the datetime value of when you started the simulation. Write the updated parquet files to the results/stream/staging directory. Move the files from the staging directory to the input directory. This is necessary to prevent Spark from reading partially written files. Your results folder should look similar to the one below right before you move the data from staging to input . results \u2514\u2500\u2500 stream \u251c\u2500\u2500 input \u2502 \u251c\u2500\u2500 accelerations \u2502 \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 t = 003 .2 . . . \u2502 \u2502 \u251c\u2500\u2500 t = 050 .6 \u2502 \u2502 \u2514\u2500\u2500 t = 051 .6 \u2502 \u2514\u2500\u2500 locations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 t = 004 .5 . . . \u2502 \u251c\u2500\u2500 t = 049 .5 \u2502 \u251c\u2500\u2500 t = 050 .6 \u2502 \u2514\u2500\u2500 t = 051 .6 \u251c\u2500\u2500 output \u2514\u2500\u2500 staging \u251c\u2500\u2500 accelerations \u2502 \u2514\u2500\u2500 t = 052 .5 \u2514\u2500\u2500 locations \u2514\u2500\u2500 t = 052 .5 When your script starts, you will probably want to remove any existing files in the staging and input directories.","title":"Assignment 8.1"},{"location":"lessons/10-week/week08/#assignment-82","text":"In the second part of the exercise, you will create two streaming dataframes using the accelerations and locations folders.","title":"Assignment 8.2"},{"location":"lessons/10-week/week08/#a","text":"Start by creating a simple Spark Streaming application that reads data from the accelerations and locations folders in results/input and uses file sink to save the results to results/output/simple .","title":"a."},{"location":"lessons/10-week/week08/#b","text":"Define a watermark on both dataframes using the timestamp column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each route. Save the results in results/output/windowed/ and set the output mode to update .","title":"b."},{"location":"lessons/10-week/week08/#c","text":"Join the two streams together on the UUID as an outer join. Save the results in results/output/stream-joined .","title":"c."},{"location":"lessons/10-week/week08/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment08/ directory. Use the naming convention of assignment08_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment08_DoeJane.zip assignment08 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment08 -DestinationPath ' assignment08_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week08/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/10-week/week08/#topic-1","text":"Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems. What other data systems make use of this data structure?","title":"Topic 1"},{"location":"lessons/10-week/week08/#topic-2","text":"Representational State Transfer REST REST is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications?","title":"Topic 2"},{"location":"lessons/10-week/week08/#topic-3","text":"Describe how different database systems handle transactions. Pick three or more different systems to compare and contrast.","title":"Topic 3"},{"location":"lessons/10-week/week09/","text":"Week 9 \u00b6 In this lesson we learn how to preprocess text-based data and train deep learning models on that data. Objectives \u00b6 After completing this week, you should be able to: Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing. Build basic text-processing models using recurrent neural networks (RNN) Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models Readings \u00b6 Read chapter 6 in Deep Learning with Python Weekly Resources \u00b6 Global Vectors for Word Representation Large Movie Review Dataset Extracting, transforming and selecting features Assignment 9 \u00b6 9.1 \u00b6 In the first part of the assignment, you will implement basic text-preprocessing functions in Python. These functions do not need to scale to large text documents and will only need to handle small inputs. a. \u00b6 Create a tokenize function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. def tokenize ( sentence ): tokens = [] # tokenize the sentence return tokens ```` #### b. Implement an ` ngram ` function that splits tokens into N - grams . ``` python def ngram ( tokens , n ): ngrams = [] # Create ngrams return ngrams c. \u00b6 Implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens. def one_hot_encode ( tokens , num_words ): token_index = {} results = '' return results 9.2 \u00b6 Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in data/external/imdb/ . Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment9/results/model_1 directory. 9.3 \u00b6 Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment9/results/model_2 directory. 9.4 \u00b6 Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment09/results/model_3 directory. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment09/ directory. Use the naming convention of assignment09_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment09_DoeJane.zip assignment09 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment09 -DestinationPath ' assignment09_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another? Topic 2 \u00b6 How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests.","title":"Week 9"},{"location":"lessons/10-week/week09/#week-9","text":"In this lesson we learn how to preprocess text-based data and train deep learning models on that data.","title":"Week 9"},{"location":"lessons/10-week/week09/#objectives","text":"After completing this week, you should be able to: Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing. Build basic text-processing models using recurrent neural networks (RNN) Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models","title":"Objectives"},{"location":"lessons/10-week/week09/#readings","text":"Read chapter 6 in Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week09/#weekly-resources","text":"Global Vectors for Word Representation Large Movie Review Dataset Extracting, transforming and selecting features","title":"Weekly Resources"},{"location":"lessons/10-week/week09/#assignment-9","text":"","title":"Assignment 9"},{"location":"lessons/10-week/week09/#91","text":"In the first part of the assignment, you will implement basic text-preprocessing functions in Python. These functions do not need to scale to large text documents and will only need to handle small inputs.","title":"9.1"},{"location":"lessons/10-week/week09/#a","text":"Create a tokenize function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. def tokenize ( sentence ): tokens = [] # tokenize the sentence return tokens ```` #### b. Implement an ` ngram ` function that splits tokens into N - grams . ``` python def ngram ( tokens , n ): ngrams = [] # Create ngrams return ngrams","title":"a."},{"location":"lessons/10-week/week09/#c","text":"Implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens. def one_hot_encode ( tokens , num_words ): token_index = {} results = '' return results","title":"c."},{"location":"lessons/10-week/week09/#92","text":"Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in data/external/imdb/ . Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment9/results/model_1 directory.","title":"9.2"},{"location":"lessons/10-week/week09/#93","text":"Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment9/results/model_2 directory.","title":"9.3"},{"location":"lessons/10-week/week09/#94","text":"Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Save the model performance metrics and training and validation accuracy curves in the dsc650/assignments/assignment09/results/model_3 directory.","title":"9.4"},{"location":"lessons/10-week/week09/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment09/ directory. Use the naming convention of assignment09_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment09_DoeJane.zip assignment09 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment09 -DestinationPath ' assignment09_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week09/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/10-week/week09/#topic-1","text":"Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another?","title":"Topic 1"},{"location":"lessons/10-week/week09/#topic-2","text":"How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests.","title":"Topic 2"},{"location":"lessons/10-week/week10/","text":"Week 10 \u00b6 In this lesson, we will explore the future of big data and deep learning. Objectives \u00b6 After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation Readings \u00b6 Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python Weekly Resources \u00b6 Assignment 10 \u00b6 Assignment 10.1 \u00b6 Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the results directory of dsc650/assignments/assignment10/ . Assignment 10.2 \u00b6 Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the results/vae directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment10/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment10 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment10_DoeJane.zip Discussion Board \u00b6 For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 10"},{"location":"lessons/10-week/week10/#week-10","text":"In this lesson, we will explore the future of big data and deep learning.","title":"Week 10"},{"location":"lessons/10-week/week10/#objectives","text":"After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation","title":"Objectives"},{"location":"lessons/10-week/week10/#readings","text":"Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python","title":"Readings"},{"location":"lessons/10-week/week10/#weekly-resources","text":"","title":"Weekly Resources"},{"location":"lessons/10-week/week10/#assignment-10","text":"","title":"Assignment 10"},{"location":"lessons/10-week/week10/#assignment-101","text":"Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the results directory of dsc650/assignments/assignment10/ .","title":"Assignment 10.1"},{"location":"lessons/10-week/week10/#assignment-102","text":"Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the results/vae directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead.","title":"Assignment 10.2"},{"location":"lessons/10-week/week10/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment10/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment10 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment10_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/10-week/week10/#discussion-board","text":"For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/","text":"Documentation in Progress Check back soon for more updates.","title":"Index"},{"location":"lessons/12-week/week01/","text":"To paraphrase the late Douglas Adams: Big data is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think your collection of movies, pictures, and music is big, but that's just peanuts to big data. Big Data is big in two distinct ways. First, as the name suggests, Big Data is about how to deal with large amounts of data. Tech giants like Google and Facebook store exabytes of data . While multiple exabytes of data is an impressive amount of data, it is nowhere near the theoretical limits . Second, Big Data is a wide area of study that spans a wide range of technologies and concepts. Because of the size and rapid rate of change of the subject, we will only be able to cover a small fraction of Big Data topics in this course. In this course, we will focus on two main areas: the design of data-driven systems and deep learning algorithms. The first area, the design of data-driven systems, takes a high-level look into how the different components of Big Data systems fit together. The second area, deep learning, focuses on a specific approach to extracting information from large, usually unstructured datasets. Objectives \u00b6 After completing this week, you should be able to: Setup a development environment PySpark, Keras, and TensorFlow and run a simple proof of concept Explain how reliability, scalability, and maintainability impacts data-driven systems Summarize how artificial intelligence, machine learning, and deep learning relate to one another Determine what problems deep learning and big data help solve Readings \u00b6 Read chapter 1 in Designing Data-Intensive Applications Read chapter 1 in Deep Learning with Python Visit DSC 650 website and follow the getting started instructions to setup your development environment Watch CPU vs GPU What's the Difference? What's the Difference? Watch How Much Information is in the Universe? | Space Time Weekly Resources \u00b6 Backblaze Hard Drive Stats DSC 650 Website DSC 650 Github Repository CPU vs GPU What's the Difference? What's the Difference? How Much Information is in the Universe? | Space Time Assignment 1 \u00b6 Assignment 1.1 \u00b6 Visit [DSC 650 website][dsc650] and follow the instructions for getting started. To demonstrate your environment is in working order, run the examples in the examples folder, and copy the output to the dsc650/assignment01/logs folder. a. Run Keras MNIST MLP Example \u00b6 If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/mnist_mlp.py > logs/keras-mnist.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. b. Run PySpark Example \u00b6 If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/pi.py > logs/spark-pi.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. Assignment 1.2 \u00b6 For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at dsc650/assignments/assignment01/Assignment 01.md . a. Data Sizes \u00b6 Provide estimates for the size of various data items. Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB Assume all videos are 30 frames per second HEVC stands for High Efficiency Video Coding See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions. b. Scaling \u00b6 Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items. Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS). By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. See the Snappy Github repository for estimates of Snappy's performance. Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day. Assume that 75% of those items are 1024x768 PNG photos. YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute. For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second. c. Reliability \u00b6 Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics . # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ?? d. Latency \u00b6 Provide estimates of the one way latency for each of the following items. Please explain how you arrived at the estimates for each item by citing references or providing calculations. One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment01/ directory. Use the naming convention of assignment01_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment01_DoeJane.zip assignment01 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment01 -DestinationPath ' assignment01_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment01 \u2502 \u251c\u2500\u2500 Assignment\\ 01.md \u2502 \u2514\u2500\u2500 logs \u2502 \u251c\u2500\u2500 keras-mnist.log \u2502 \u2514\u2500\u2500 spark-pi.log Discussion \u00b6 For the first discussion, write a 250 to 750-word discussion board post about a Big Data and/or deep learning use case. Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 1"},{"location":"lessons/12-week/week01/#objectives","text":"After completing this week, you should be able to: Setup a development environment PySpark, Keras, and TensorFlow and run a simple proof of concept Explain how reliability, scalability, and maintainability impacts data-driven systems Summarize how artificial intelligence, machine learning, and deep learning relate to one another Determine what problems deep learning and big data help solve","title":"Objectives"},{"location":"lessons/12-week/week01/#readings","text":"Read chapter 1 in Designing Data-Intensive Applications Read chapter 1 in Deep Learning with Python Visit DSC 650 website and follow the getting started instructions to setup your development environment Watch CPU vs GPU What's the Difference? What's the Difference? Watch How Much Information is in the Universe? | Space Time","title":"Readings"},{"location":"lessons/12-week/week01/#weekly-resources","text":"Backblaze Hard Drive Stats DSC 650 Website DSC 650 Github Repository CPU vs GPU What's the Difference? What's the Difference? How Much Information is in the Universe? | Space Time","title":"Weekly Resources"},{"location":"lessons/12-week/week01/#assignment-1","text":"","title":"Assignment 1"},{"location":"lessons/12-week/week01/#assignment-11","text":"Visit [DSC 650 website][dsc650] and follow the instructions for getting started. To demonstrate your environment is in working order, run the examples in the examples folder, and copy the output to the dsc650/assignment01/logs folder.","title":"Assignment 1.1"},{"location":"lessons/12-week/week01/#a-run-keras-mnist-mlp-example","text":"If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/mnist_mlp.py > logs/keras-mnist.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor.","title":"a.  Run Keras MNIST MLP Example"},{"location":"lessons/12-week/week01/#b-run-pyspark-example","text":"If you are using Bash the following commands will write both stdout and stderr to a file. $ python examples/pi.py > logs/spark-pi.log 2 > & 1 If you are not using Bash, you can manually copy and paste the output into the log file using a text editor.","title":"b. Run PySpark Example"},{"location":"lessons/12-week/week01/#assignment-12","text":"For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at dsc650/assignments/assignment01/Assignment 01.md .","title":"Assignment 1.2"},{"location":"lessons/12-week/week01/#a-data-sizes","text":"Provide estimates for the size of various data items. Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB Assume all videos are 30 frames per second HEVC stands for High Efficiency Video Coding See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions.","title":"a. Data Sizes"},{"location":"lessons/12-week/week01/#b-scaling","text":"Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items. Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS). By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. See the Snappy Github repository for estimates of Snappy's performance. Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day. Assume that 75% of those items are 1024x768 PNG photos. YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute. For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second.","title":"b. Scaling"},{"location":"lessons/12-week/week01/#c-reliability","text":"Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics . # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ??","title":"c. Reliability"},{"location":"lessons/12-week/week01/#d-latency","text":"Provide estimates of the one way latency for each of the following items. Please explain how you arrived at the estimates for each item by citing references or providing calculations. One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes","title":"d. Latency"},{"location":"lessons/12-week/week01/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment01/ directory. Use the naming convention of assignment01_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment01_DoeJane.zip assignment01 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment01 -DestinationPath ' assignment01_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment01 \u2502 \u251c\u2500\u2500 Assignment\\ 01.md \u2502 \u2514\u2500\u2500 logs \u2502 \u251c\u2500\u2500 keras-mnist.log \u2502 \u2514\u2500\u2500 spark-pi.log","title":"Submission Instructions"},{"location":"lessons/12-week/week01/#discussion","text":"For the first discussion, write a 250 to 750-word discussion board post about a Big Data and/or deep learning use case. Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/12-week/week02/","text":"Week 2 \u00b6 In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms. Objectives \u00b6 After completing this week, you should be able to: Query and process data using multiple paradigms including graph processing, map-reduce, and SQL Compare and contrast different data models including identifying prime use cases for different data models Demonstrate how to represent data as tensors and apply tensor mathematical operations Readings \u00b6 Read chapters 2 and 3 in Designing Data-Intensive Applications Read chapter 2 in Deep Learning with Python Weekly Resources \u00b6 TinyDB OrientDB Getting Started OrientDB Download Keras Multi-Dimensional Data (as used in Tensors) SQL Tutorial TensorFlow Quickstart Assignment 2 \u00b6 For this assignment, we will be working with the CSV data found in the data/external/tidynomicon folder. Specifically, we will be using with the measurements.csv , person.csv , site.csv , and visited.csv files. If you are running on JupyterHub hosted on the BU Data Science Cluster, you can load data from the cluster's Amazon S3-compatible data storage. The following code demonstrates how to load the site.csv data into a Pandas dataframe. import pandas as pd import s3fs s3 = s3fs . S3FileSystem ( anon = True , client_kwargs = { 'endpoint_url' : 'https://storage.budsc.midwest-datascience.com' } ) df = pd . read_csv ( s3 . open ( 'data/external/tidynomicon/site.csv' , mode = 'rb' ) ) The other files have the same names as the files located in the repositories data/external/tidynomicon folder. Assignment 2.1 \u00b6 Complete the code in kvdb.ipynb to implement a basic key-value database that saves its state to a json file. Use that code to create databases that store each of CSV files by key. The json files should be stored in the dsc650/assignments/assignment02/results/kvdb/ folder. Input File Output File Key measurements.csv measurements.json Composite key person.csv people.json person_id site.csv sites.json site_id visited.csv visits.json Composite key The measurements.csv and visited.csv have composite keys that use multiple columns. For measurements.csv those fields are visit_id , person_id , and quantity . For visited.csv those fields are visit_id and site_id . The following is an example of code that sets and gets the value using a composite key. kvdb_path = 'visits.json' kvdb = KVDB ( kvdb_path ) key = ( 619 , 'DR-1' ) value = dict ( visit_id = 619 , site_id = 'DR-1' , visit_date = '1927-02-08' ) kvdb . set_value ( key , value ) retrieved_value = kvdb . get_value ( key ) # Retrieved should be the same as value Assignment 2.2 \u00b6 Now we will create a simple document database using the tinydb library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in dsc650/assignments/assignment02/results/patient-info.json . You will store a document for each person in the database which should look like this. { \"person_id\" : \"dyer\" , \"personal_name\" : \"William\" , \"family_name\" : \"Dyer\" , \"visits\" : [ { \"visit_id\" : 619 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-08\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 9.82 }, { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.13 } ] }, { \"visit_id\" : 622 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-10\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 7.8 }, { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.09 } ] } ] } The dsc650/assignments/assignment02/documentdb.ipynb file contains code that should assist you in this task. Assignment 2.3 \u00b6 In this part, you will create a SQLite database that you will store in dsc650/assignments/assignment02/results/patient-info.db . The dsc650/assignments/assignment02/rdbms.ipynb file should contain code to assist you in the creation of this database. Assignment 2.4 \u00b6 Go to the Wikidata Query Service website and perform the following SPARQL query. #Recent Events SELECT ?event ?eventLabel ?date WHERE { # find events ?event wdt : P31 / wdt : P279 * wd : Q1190554 . # with a point in time or start date OPTIONAL { ?event wdt : P585 ?date . } OPTIONAL { ?event wdt : P580 ?date . } # but at least one of those FILTER ( BOUND ( ?date ) && DATATYPE ( ?date ) = xsd : dateTime ). # not in the future, and not more than 31 days ago BIND ( NOW () - ?date AS ?distance ). FILTER ( 0 <= ?distance && ?distance < 31 ). # and get a label as well OPTIONAL { ?event rdfs : label ?eventLabel . FILTER ( LANG ( ?eventLabel ) = \"en\" ). } } # limit to 10 results so we don't timeout LIMIT 10 Modify the query so that the column order is date , event , and eventLabel instead of event , eventLabel , and date . Download the results as a JSON file and copy the results to dsc650/assignments/assignment02/results/wikidata-query.json . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment02/ directory. Use the naming convention of assignment02_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive on your local machine using Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment02_DoeJane.zip assignment02 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment02 -DestinationPath ' assignment02_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment02 \u2502 \u251c\u2500\u2500 documentdb.ipynb \u2502 \u251c\u2500\u2500 kvdb.ipynb \u2502 \u251c\u2500\u2500 rdbms.ipynb \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u251c\u2500\u2500 kvdb \u2502 \u2502 \u2502 \u251c\u2500\u2500 measurements.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 people.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 sites.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 visits.json \u2502 \u2502 \u251c\u2500\u2500 patient-info.db \u2502 \u2502 \u251c\u2500\u2500 patient-info.fs \u2502 \u2502 \u251c\u2500\u2500 patient-info.json \u2502 \u2502 \u2514\u2500\u2500 wikidata-query.json Discussion \u00b6 For this discussion, write a 250 to 750-word discussion board post about use cases from different data models. As an example, how could you use a graph database in one of your professional or personal projects? Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 2"},{"location":"lessons/12-week/week02/#week-2","text":"In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms.","title":"Week 2"},{"location":"lessons/12-week/week02/#objectives","text":"After completing this week, you should be able to: Query and process data using multiple paradigms including graph processing, map-reduce, and SQL Compare and contrast different data models including identifying prime use cases for different data models Demonstrate how to represent data as tensors and apply tensor mathematical operations","title":"Objectives"},{"location":"lessons/12-week/week02/#readings","text":"Read chapters 2 and 3 in Designing Data-Intensive Applications Read chapter 2 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week02/#weekly-resources","text":"TinyDB OrientDB Getting Started OrientDB Download Keras Multi-Dimensional Data (as used in Tensors) SQL Tutorial TensorFlow Quickstart","title":"Weekly Resources"},{"location":"lessons/12-week/week02/#assignment-2","text":"For this assignment, we will be working with the CSV data found in the data/external/tidynomicon folder. Specifically, we will be using with the measurements.csv , person.csv , site.csv , and visited.csv files. If you are running on JupyterHub hosted on the BU Data Science Cluster, you can load data from the cluster's Amazon S3-compatible data storage. The following code demonstrates how to load the site.csv data into a Pandas dataframe. import pandas as pd import s3fs s3 = s3fs . S3FileSystem ( anon = True , client_kwargs = { 'endpoint_url' : 'https://storage.budsc.midwest-datascience.com' } ) df = pd . read_csv ( s3 . open ( 'data/external/tidynomicon/site.csv' , mode = 'rb' ) ) The other files have the same names as the files located in the repositories data/external/tidynomicon folder.","title":"Assignment 2"},{"location":"lessons/12-week/week02/#assignment-21","text":"Complete the code in kvdb.ipynb to implement a basic key-value database that saves its state to a json file. Use that code to create databases that store each of CSV files by key. The json files should be stored in the dsc650/assignments/assignment02/results/kvdb/ folder. Input File Output File Key measurements.csv measurements.json Composite key person.csv people.json person_id site.csv sites.json site_id visited.csv visits.json Composite key The measurements.csv and visited.csv have composite keys that use multiple columns. For measurements.csv those fields are visit_id , person_id , and quantity . For visited.csv those fields are visit_id and site_id . The following is an example of code that sets and gets the value using a composite key. kvdb_path = 'visits.json' kvdb = KVDB ( kvdb_path ) key = ( 619 , 'DR-1' ) value = dict ( visit_id = 619 , site_id = 'DR-1' , visit_date = '1927-02-08' ) kvdb . set_value ( key , value ) retrieved_value = kvdb . get_value ( key ) # Retrieved should be the same as value","title":"Assignment 2.1"},{"location":"lessons/12-week/week02/#assignment-22","text":"Now we will create a simple document database using the tinydb library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in dsc650/assignments/assignment02/results/patient-info.json . You will store a document for each person in the database which should look like this. { \"person_id\" : \"dyer\" , \"personal_name\" : \"William\" , \"family_name\" : \"Dyer\" , \"visits\" : [ { \"visit_id\" : 619 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-08\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 9.82 }, { \"visit_id\" : 619 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.13 } ] }, { \"visit_id\" : 622 , \"site_id\" : \"DR-1\" , \"visit_date\" : \"1927-02-10\" , \"site\" : { \"site_id\" : \"DR-1\" , \"latitude\" : -49.85 , \"longitude\" : -128.57 }, \"measurements\" : [ { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"rad\" , \"reading\" : 7.8 }, { \"visit_id\" : 622 , \"person_id\" : \"dyer\" , \"quantity\" : \"sal\" , \"reading\" : 0.09 } ] } ] } The dsc650/assignments/assignment02/documentdb.ipynb file contains code that should assist you in this task.","title":"Assignment 2.2"},{"location":"lessons/12-week/week02/#assignment-23","text":"In this part, you will create a SQLite database that you will store in dsc650/assignments/assignment02/results/patient-info.db . The dsc650/assignments/assignment02/rdbms.ipynb file should contain code to assist you in the creation of this database.","title":"Assignment 2.3"},{"location":"lessons/12-week/week02/#assignment-24","text":"Go to the Wikidata Query Service website and perform the following SPARQL query. #Recent Events SELECT ?event ?eventLabel ?date WHERE { # find events ?event wdt : P31 / wdt : P279 * wd : Q1190554 . # with a point in time or start date OPTIONAL { ?event wdt : P585 ?date . } OPTIONAL { ?event wdt : P580 ?date . } # but at least one of those FILTER ( BOUND ( ?date ) && DATATYPE ( ?date ) = xsd : dateTime ). # not in the future, and not more than 31 days ago BIND ( NOW () - ?date AS ?distance ). FILTER ( 0 <= ?distance && ?distance < 31 ). # and get a label as well OPTIONAL { ?event rdfs : label ?eventLabel . FILTER ( LANG ( ?eventLabel ) = \"en\" ). } } # limit to 10 results so we don't timeout LIMIT 10 Modify the query so that the column order is date , event , and eventLabel instead of event , eventLabel , and date . Download the results as a JSON file and copy the results to dsc650/assignments/assignment02/results/wikidata-query.json .","title":"Assignment 2.4"},{"location":"lessons/12-week/week02/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment02/ directory. Use the naming convention of assignment02_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive on your local machine using Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment02_DoeJane.zip assignment02 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment02 -DestinationPath ' assignment02_DoeJane.zip When decompressed, the output should have the following directory structure. \u251c\u2500\u2500 assignment02 \u2502 \u251c\u2500\u2500 documentdb.ipynb \u2502 \u251c\u2500\u2500 kvdb.ipynb \u2502 \u251c\u2500\u2500 rdbms.ipynb \u2502 \u251c\u2500\u2500 results \u2502 \u2502 \u251c\u2500\u2500 kvdb \u2502 \u2502 \u2502 \u251c\u2500\u2500 measurements.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 people.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 sites.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 visits.json \u2502 \u2502 \u251c\u2500\u2500 patient-info.db \u2502 \u2502 \u251c\u2500\u2500 patient-info.fs \u2502 \u2502 \u251c\u2500\u2500 patient-info.json \u2502 \u2502 \u2514\u2500\u2500 wikidata-query.json","title":"Submission Instructions"},{"location":"lessons/12-week/week02/#discussion","text":"For this discussion, write a 250 to 750-word discussion board post about use cases from different data models. As an example, how could you use a graph database in one of your professional or personal projects? Try to focus on a use case relevant to your professional or personal interests. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/12-week/week03/","text":"Week 3 \u00b6 In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes. Objectives \u00b6 After completing this week, you should be able to: Compare indexing algorithms including hash indexes and B-Trees Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution Readings \u00b6 Read chapters 3 and 4 in Designing Data-Intensive Applications Weekly Resources \u00b6 Apache Arrow Apache Parquet Apache Thrift Apache Avro JSON Schema Protocol Buffers Assignment 3 \u00b6 For this assignment, you will be working with data from OpenFlights . This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in data/external/openflights/ . For this assignment, you will use a dataset derived from that original data. You can find this data in data/processed/openflights/routes.jsonl.gz . The data is compressed with gzip and encoded in the JSON Lines format . Each line represents a single airline route. The dsc650/assignments/assignment03 directory contains placeholder code and data outputs for this assignment. Assignment 3.1 \u00b6 In the first part of the assignment, you will be creating schemas for the route data and encoding the routes.jsonl.gz using Protocol Buffers, Avro, and Parquet. a. JSON Schema \u00b6 Create a JSON Schema in the schemas/routes-schema.json file to describe a route and validate the data in routes.jsonl.gz using the jsonschema library. b. Avro \u00b6 Use the fastavro library to create results/routes.avro with the schema provided. c. Parquet \u00b6 Create a Parquet dataset in results/routes.parquet using Apache Arrow . d. Protocol Buffers \u00b6 Using the generated code found in dsc650/assignment/assignment03/routes_pb2.py create results/routes.pb using Protocol Buffers. e. Output Sizes \u00b6 Compare the output sizes of the different formats. Populate the results in results/comparison.csv . Compare compressed and uncompressed sizes if possible. Assignment 3.2 \u00b6 This part of the assignment involves developing a rudimentary database index for our routes dataset. Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash . Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer. Below is an example of the geohashed value for Bellevue University. import pygeohash pygeohash . encode ( 41.1499988 , - 95.91779 ) '9z7f174u17zb' Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. The following table gives cell width and height values for each level of precision of the geohash. Geohash Coordinates Cell Width 1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m As you can see, it only takes about four levels characters to get to a 40 km by 20 km area. Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy. a. Create a Simple Geohash Index \u00b6 Using pygeohash create a simple index for the routes.jsonl.gz data using the source airport latitude and longitude. Output the index and values to the results/geoindex directory. The output looks like the following directory structure. geoindex \u251c\u2500\u2500 2 \u2502 \u251c\u2500\u2500 2e \u2502 \u2502 \u251c\u2500\u2500 2eg.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2ev.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2ey.jsonl.gz \u2502 \u251c\u2500\u2500 2h \u2502 \u2502 \u251c\u2500\u2500 2h5.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2hb.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2hx.jsonl.gz \u2502 \u251c\u2500\u2500 2j \u2502 \u2502 \u251c\u2500\u2500 2j0.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2j3.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2jd.jsonl.gz . . . \u2502 \u2514\u2500\u2500 yu \u2502 \u2514\u2500\u2500 yue.jsonl.gz \u2514\u2500\u2500 z \u251c\u2500\u2500 z0 \u2502 \u251c\u2500\u2500 z08.jsonl.gz \u2502 \u251c\u2500\u2500 z0h.jsonl.gz \u2502 \u2514\u2500\u2500 z0m.jsonl.gz \u251c\u2500\u2500 z6 \u2502 \u2514\u2500\u2500 z6e.jsonl.gz \u251c\u2500\u2500 z9 \u2502 \u2514\u2500\u2500 z92.jsonl.gz \u251c\u2500\u2500 zg \u2502 \u2514\u2500\u2500 zgw.jsonl.gz \u251c\u2500\u2500 zk \u2502 \u2514\u2500\u2500 zk9.jsonl.gz \u251c\u2500\u2500 zs \u2502 \u2514\u2500\u2500 zs4.jsonl.gz \u2514\u2500\u2500 zu \u2514\u2500\u2500 zu3.jsonl.gz b. Implement a Simple Search Feature \u00b6 Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the geohash_approximate_distance function in pygeohash to compute distances between geohash values. It returns distances in meters, but your search function should use kilometers as input. import pygeohash pygeohash . geohash_approximate_distance ( 'bcd3u' , 'bc83n' ) # >>> 625441 Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment03/ directory. Use the naming convention of assignment03_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment03_DoeJane.zip assignment03 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment03 -DestinationPath ' assignment03_DoeJane.zip Discussion \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database? Topic 2 \u00b6 Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format. Topic 3 \u00b6 Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data? Topic 4 \u00b6 We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9","title":"Week 3"},{"location":"lessons/12-week/week03/#week-3","text":"In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes.","title":"Week 3"},{"location":"lessons/12-week/week03/#objectives","text":"After completing this week, you should be able to: Compare indexing algorithms including hash indexes and B-Trees Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution","title":"Objectives"},{"location":"lessons/12-week/week03/#readings","text":"Read chapters 3 and 4 in Designing Data-Intensive Applications","title":"Readings"},{"location":"lessons/12-week/week03/#weekly-resources","text":"Apache Arrow Apache Parquet Apache Thrift Apache Avro JSON Schema Protocol Buffers","title":"Weekly Resources"},{"location":"lessons/12-week/week03/#assignment-3","text":"For this assignment, you will be working with data from OpenFlights . This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in data/external/openflights/ . For this assignment, you will use a dataset derived from that original data. You can find this data in data/processed/openflights/routes.jsonl.gz . The data is compressed with gzip and encoded in the JSON Lines format . Each line represents a single airline route. The dsc650/assignments/assignment03 directory contains placeholder code and data outputs for this assignment.","title":"Assignment 3"},{"location":"lessons/12-week/week03/#assignment-31","text":"In the first part of the assignment, you will be creating schemas for the route data and encoding the routes.jsonl.gz using Protocol Buffers, Avro, and Parquet.","title":"Assignment 3.1"},{"location":"lessons/12-week/week03/#a-json-schema","text":"Create a JSON Schema in the schemas/routes-schema.json file to describe a route and validate the data in routes.jsonl.gz using the jsonschema library.","title":"a. JSON Schema"},{"location":"lessons/12-week/week03/#b-avro","text":"Use the fastavro library to create results/routes.avro with the schema provided.","title":"b. Avro"},{"location":"lessons/12-week/week03/#c-parquet","text":"Create a Parquet dataset in results/routes.parquet using Apache Arrow .","title":"c. Parquet"},{"location":"lessons/12-week/week03/#d-protocol-buffers","text":"Using the generated code found in dsc650/assignment/assignment03/routes_pb2.py create results/routes.pb using Protocol Buffers.","title":"d. Protocol Buffers"},{"location":"lessons/12-week/week03/#e-output-sizes","text":"Compare the output sizes of the different formats. Populate the results in results/comparison.csv . Compare compressed and uncompressed sizes if possible.","title":"e. Output Sizes"},{"location":"lessons/12-week/week03/#assignment-32","text":"This part of the assignment involves developing a rudimentary database index for our routes dataset. Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash . Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer. Below is an example of the geohashed value for Bellevue University. import pygeohash pygeohash . encode ( 41.1499988 , - 95.91779 ) '9z7f174u17zb' Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. The following table gives cell width and height values for each level of precision of the geohash. Geohash Coordinates Cell Width 1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m As you can see, it only takes about four levels characters to get to a 40 km by 20 km area. Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy.","title":"Assignment 3.2"},{"location":"lessons/12-week/week03/#a-create-a-simple-geohash-index","text":"Using pygeohash create a simple index for the routes.jsonl.gz data using the source airport latitude and longitude. Output the index and values to the results/geoindex directory. The output looks like the following directory structure. geoindex \u251c\u2500\u2500 2 \u2502 \u251c\u2500\u2500 2e \u2502 \u2502 \u251c\u2500\u2500 2eg.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2ev.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2ey.jsonl.gz \u2502 \u251c\u2500\u2500 2h \u2502 \u2502 \u251c\u2500\u2500 2h5.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2hb.jsonl.gz \u2502 \u2502 \u2514\u2500\u2500 2hx.jsonl.gz \u2502 \u251c\u2500\u2500 2j \u2502 \u2502 \u251c\u2500\u2500 2j0.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2j3.jsonl.gz \u2502 \u2502 \u251c\u2500\u2500 2jd.jsonl.gz . . . \u2502 \u2514\u2500\u2500 yu \u2502 \u2514\u2500\u2500 yue.jsonl.gz \u2514\u2500\u2500 z \u251c\u2500\u2500 z0 \u2502 \u251c\u2500\u2500 z08.jsonl.gz \u2502 \u251c\u2500\u2500 z0h.jsonl.gz \u2502 \u2514\u2500\u2500 z0m.jsonl.gz \u251c\u2500\u2500 z6 \u2502 \u2514\u2500\u2500 z6e.jsonl.gz \u251c\u2500\u2500 z9 \u2502 \u2514\u2500\u2500 z92.jsonl.gz \u251c\u2500\u2500 zg \u2502 \u2514\u2500\u2500 zgw.jsonl.gz \u251c\u2500\u2500 zk \u2502 \u2514\u2500\u2500 zk9.jsonl.gz \u251c\u2500\u2500 zs \u2502 \u2514\u2500\u2500 zs4.jsonl.gz \u2514\u2500\u2500 zu \u2514\u2500\u2500 zu3.jsonl.gz","title":"a. Create a Simple Geohash Index"},{"location":"lessons/12-week/week03/#b-implement-a-simple-search-feature","text":"Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the geohash_approximate_distance function in pygeohash to compute distances between geohash values. It returns distances in meters, but your search function should use kilometers as input. import pygeohash pygeohash . geohash_approximate_distance ( 'bcd3u' , 'bc83n' ) # >>> 625441","title":"b. Implement a Simple Search Feature"},{"location":"lessons/12-week/week03/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment03/ directory. Use the naming convention of assignment03_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment03_DoeJane.zip assignment03 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment03 -DestinationPath ' assignment03_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week03/#discussion","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion"},{"location":"lessons/12-week/week03/#topic-1","text":"Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database?","title":"Topic 1"},{"location":"lessons/12-week/week03/#topic-2","text":"Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format.","title":"Topic 2"},{"location":"lessons/12-week/week03/#topic-3","text":"Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data?","title":"Topic 3"},{"location":"lessons/12-week/week03/#topic-4","text":"We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9","title":"Topic 4"},{"location":"lessons/12-week/week04/","text":"Week 4 \u00b6 In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks. Objectives \u00b6 After completing this week, you should be able to: Create a scalable, end-to-end machine learning pipeline using Spark Readings \u00b6 Read chapter 10 in Designing Data-Intensive Applications Read chapter 3 in Deep Learning with Python Weekly Resources \u00b6 Celery Architecture Kubernetes Architecture HDFS Architecture YARN Architecture Assignment 4 \u00b6 In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in email package to read the emails and create a machine-friendly dataset. The data/external/enron folder contains a partial copy of the original Enron email dataset (you can download the full dataset here ). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. enron/zipper-a \u251c\u2500\u2500 all_documents \u2502 \u251c\u2500\u2500 1 . \u2502 \u251c\u2500\u2500 10 . \u2502 \u251c\u2500\u2500 11 . \u2502 \u251c\u2500\u2500 12 . \u2502 \u251c\u2500\u2500 13 . \u2502 \u251c\u2500\u2500 14 . . . . \u2502 \u251c\u2500\u2500 8 . \u2502 \u2514\u2500\u2500 9 . \u2514\u2500\u2500 tss \u251c\u2500\u2500 1 . \u251c\u2500\u2500 10 . \u251c\u2500\u2500 11 . \u251c\u2500\u2500 12 . . . . \u251c\u2500\u2500 4 . \u251c\u2500\u2500 5 . \u251c\u2500\u2500 6 . \u251c\u2500\u2500 7 . \u251c\u2500\u2500 8 . \u2514\u2500\u2500 9 . Looking at the example of /enron/zipper-a/inbox/114. demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body. This is typical of most of the emails except some email bodies being encoded in HTML. Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header The dsc650/assignments/assignment04 folder contains partially completed code and placeholder files for this assignment. Assignment 4.1 \u00b6 Load the data from the enron.zip dataset into a Spark dataframe. See the Spark SQL Getting Started Guide for information details on how to create a dataframe. The final dataframe should have the following schema. df . printSchema () root |-- id : string ( nullable = true ) |-- username : string ( nullable = true ) |-- original_msg : string ( nullable = true ) Assignment 4.2 \u00b6 Implement a function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. The folder dsc650/assignments/assignment04/examples contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your read_email function is working properly. The following is an example of parsing a plain email message. plain_msg_example = \"\"\" Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad \"\"\" parsed_msg = parse_email ( plain_msg_example ) print ( parsed_msg . text ) Which yields the following. Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad Assignment 4.3 \u00b6 Finally, you will put together a feature extraction workflow using Spark Pipelines . You will use Spark's MLlib to extract words from the email text and then convert those words into numeric features using a count vectorizor. result . select ( 'id' , 'words' , 'features' ) . show () +--------------------+--------------------+--------------------+ | id | words | features | +--------------------+--------------------+--------------------+ | shively - h / 2 _ | [, what , is , this ...| ( 3 ,[ 0 ],[ 16.0 ]) | | shively - h / 1 _ | [ can , you , please ...| ( 3 ,[ 0 , 1 , 2 ],[ 5.0 , 1. ..| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 51.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 53.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 96.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 13.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 13.0 , ...| | shively - h / inbox / 60 _ | [ i , just , wanted , ...| ( 3 ,[ 0 , 1 , 2 ],[ 47.0 , ...| | shively - h / inbox / 134 _ | [ this , is , an , al ...| ( 3 ,[ 0 , 1 , 2 ],[ 6.0 , 3. ..| | shively - h / inbox / 70 _ | [, please , find , ...| ( 3 ,[ 0 , 1 , 2 ],[ 8.0 , 7. ..| | shively - h / inbox / 118 _ | [ frank , ermis , - , ...| ( 3 ,[ 0 , 2 ],[ 9.0 , 1.0 ]) | | shively - h / inbox / 28 _ | [ dear , body , shop ...| ( 3 ,[ 0 , 1 , 2 ],[ 8.0 , 3. ..| | shively - h / inbox / 178 _ | [, as , you , know , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / inbox / 73 _ | [ hunter , shively , ...| ( 3 ,[ 0 , 1 ],[ 13.0 , 3.0 ]) | | shively - h / inbox / 7 _ | [ hunter , -- , this ...| ( 3 ,[ 0 , 1 , 2 ],[ 11.0 , ...| | shively - h / inbox / 31 _ | [ this , is , very , ...| ( 3 ,[ 0 , 1 , 2 ],[ 27.0 , ...| | shively - h / inbox / 74 _ | [, , , ----- origi ...| ( 3 ,[ 0 , 1 , 2 ],[ 33.0 , ...| | shively - h / inbox / 97 _ | [ hunter ,, , i , wi ...| ( 3 ,[ 0 , 1 , 2 ],[ 11.0 , ...| +--------------------+--------------------+--------------------+ only showing top 20 rows Submission Instructions \u00b6 If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment04_DoeJane.zip assignment04 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment04 -DestinationPath ' assignment04_DoeJane.zip Discussion \u00b6 For this discussion, describe a batch workflow use case that you would run on a daily, weekly, or monthly basis. What are the inputs and the outputs?","title":"Week 4"},{"location":"lessons/12-week/week04/#week-4","text":"In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks.","title":"Week 4"},{"location":"lessons/12-week/week04/#objectives","text":"After completing this week, you should be able to: Create a scalable, end-to-end machine learning pipeline using Spark","title":"Objectives"},{"location":"lessons/12-week/week04/#readings","text":"Read chapter 10 in Designing Data-Intensive Applications Read chapter 3 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week04/#weekly-resources","text":"Celery Architecture Kubernetes Architecture HDFS Architecture YARN Architecture","title":"Weekly Resources"},{"location":"lessons/12-week/week04/#assignment-4","text":"In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in email package to read the emails and create a machine-friendly dataset. The data/external/enron folder contains a partial copy of the original Enron email dataset (you can download the full dataset here ). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. enron/zipper-a \u251c\u2500\u2500 all_documents \u2502 \u251c\u2500\u2500 1 . \u2502 \u251c\u2500\u2500 10 . \u2502 \u251c\u2500\u2500 11 . \u2502 \u251c\u2500\u2500 12 . \u2502 \u251c\u2500\u2500 13 . \u2502 \u251c\u2500\u2500 14 . . . . \u2502 \u251c\u2500\u2500 8 . \u2502 \u2514\u2500\u2500 9 . \u2514\u2500\u2500 tss \u251c\u2500\u2500 1 . \u251c\u2500\u2500 10 . \u251c\u2500\u2500 11 . \u251c\u2500\u2500 12 . . . . \u251c\u2500\u2500 4 . \u251c\u2500\u2500 5 . \u251c\u2500\u2500 6 . \u251c\u2500\u2500 7 . \u251c\u2500\u2500 8 . \u2514\u2500\u2500 9 . Looking at the example of /enron/zipper-a/inbox/114. demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body. This is typical of most of the emails except some email bodies being encoded in HTML. Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header The dsc650/assignments/assignment04 folder contains partially completed code and placeholder files for this assignment.","title":"Assignment 4"},{"location":"lessons/12-week/week04/#assignment-41","text":"Load the data from the enron.zip dataset into a Spark dataframe. See the Spark SQL Getting Started Guide for information details on how to create a dataframe. The final dataframe should have the following schema. df . printSchema () root |-- id : string ( nullable = true ) |-- username : string ( nullable = true ) |-- original_msg : string ( nullable = true )","title":"Assignment 4.1"},{"location":"lessons/12-week/week04/#assignment-42","text":"Implement a function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. The folder dsc650/assignments/assignment04/examples contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your read_email function is working properly. The following is an example of parsing a plain email message. plain_msg_example = \"\"\" Message-ID: <6742786.1075845426893.JavaMail.evans@thyme> Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT) From: jeffrey.hammad@enron.com To: andy.zipper@enron.com Subject: Thanks for the interview Mime-Version: 1.0 Content-Type: text/plain; charset=us-ascii Content-Transfer-Encoding: 7bit X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7> X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER> X-cc: X-bcc: X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox X-Origin: ZIPPER-A X-FileName: Zipper, Andy.pst Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad \"\"\" parsed_msg = parse_email ( plain_msg_example ) print ( parsed_msg . text ) Which yields the following. Andy, Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program. I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. Thanks and Best Regards, Jeff Hammad","title":"Assignment 4.2"},{"location":"lessons/12-week/week04/#assignment-43","text":"Finally, you will put together a feature extraction workflow using Spark Pipelines . You will use Spark's MLlib to extract words from the email text and then convert those words into numeric features using a count vectorizor. result . select ( 'id' , 'words' , 'features' ) . show () +--------------------+--------------------+--------------------+ | id | words | features | +--------------------+--------------------+--------------------+ | shively - h / 2 _ | [, what , is , this ...| ( 3 ,[ 0 ],[ 16.0 ]) | | shively - h / 1 _ | [ can , you , please ...| ( 3 ,[ 0 , 1 , 2 ],[ 5.0 , 1. ..| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 51.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 53.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / peoples ...| [, pgl , and , nort ...| ( 3 ,[ 0 , 1 , 2 ],[ 96.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 13.0 , ...| | shively - h / peoples ...| [ pgl , and , north , ...| ( 3 ,[ 0 , 1 , 2 ],[ 13.0 , ...| | shively - h / inbox / 60 _ | [ i , just , wanted , ...| ( 3 ,[ 0 , 1 , 2 ],[ 47.0 , ...| | shively - h / inbox / 134 _ | [ this , is , an , al ...| ( 3 ,[ 0 , 1 , 2 ],[ 6.0 , 3. ..| | shively - h / inbox / 70 _ | [, please , find , ...| ( 3 ,[ 0 , 1 , 2 ],[ 8.0 , 7. ..| | shively - h / inbox / 118 _ | [ frank , ermis , - , ...| ( 3 ,[ 0 , 2 ],[ 9.0 , 1.0 ]) | | shively - h / inbox / 28 _ | [ dear , body , shop ...| ( 3 ,[ 0 , 1 , 2 ],[ 8.0 , 3. ..| | shively - h / inbox / 178 _ | [, as , you , know , ...| ( 3 ,[ 0 , 1 , 2 ],[ 10.0 , ...| | shively - h / inbox / 73 _ | [ hunter , shively , ...| ( 3 ,[ 0 , 1 ],[ 13.0 , 3.0 ]) | | shively - h / inbox / 7 _ | [ hunter , -- , this ...| ( 3 ,[ 0 , 1 , 2 ],[ 11.0 , ...| | shively - h / inbox / 31 _ | [ this , is , very , ...| ( 3 ,[ 0 , 1 , 2 ],[ 27.0 , ...| | shively - h / inbox / 74 _ | [, , , ----- origi ...| ( 3 ,[ 0 , 1 , 2 ],[ 33.0 , ...| | shively - h / inbox / 97 _ | [ hunter ,, , i , wi ...| ( 3 ,[ 0 , 1 , 2 ],[ 11.0 , ...| +--------------------+--------------------+--------------------+ only showing top 20 rows","title":"Assignment 4.3"},{"location":"lessons/12-week/week04/#submission-instructions","text":"If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment04_DoeJane.zip assignment04 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment04 -DestinationPath ' assignment04_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week04/#discussion","text":"For this discussion, describe a batch workflow use case that you would run on a daily, weekly, or monthly basis. What are the inputs and the outputs?","title":"Discussion"},{"location":"lessons/12-week/week05/","text":"Week 5 \u00b6 ]( https://hackmd.io/BEx2SHHWQXGvtj-9WBlGTA ) In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python . This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects. Objectives \u00b6 After completing this week, you should be able to: Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression Create workflows that train deep learning models and then produce validation and metrics on those models Readings \u00b6 Read chapters 3 and 4 Deep Learning with Python Weekly Resources \u00b6 Assignment 5 \u00b6 In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python . You will use that code to create a workflow that trains the model, uses the model to perform model validation, and output model metrics. Assignment 5.1 \u00b6 Implement the movie review classifier found in section 3.4 of Deep Learning with Python . Assignment 5.2 \u00b6 Implement the news classifier found in section 3.5 of Deep Learning with Python . Assignment 5.3 \u00b6 Implement the housing price regression model found in section 3.6 of Deep Learning with Python . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment05/ directory. Use the naming convention of assignment05_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment05_DoeJane.zip assignment05 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment05 -DestinationPath ' assignment05_DoeJane.zip Discussion Board \u00b6 For this discussion, write a 250 to 750-word discussion board post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests. In this use case, how often would you need to train the models? How would you deploy the models? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Week 5"},{"location":"lessons/12-week/week05/#week-5","text":"]( https://hackmd.io/BEx2SHHWQXGvtj-9WBlGTA ) In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python . This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects.","title":"Week 5"},{"location":"lessons/12-week/week05/#objectives","text":"After completing this week, you should be able to: Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression Create workflows that train deep learning models and then produce validation and metrics on those models","title":"Objectives"},{"location":"lessons/12-week/week05/#readings","text":"Read chapters 3 and 4 Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week05/#weekly-resources","text":"","title":"Weekly Resources"},{"location":"lessons/12-week/week05/#assignment-5","text":"In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python . You will use that code to create a workflow that trains the model, uses the model to perform model validation, and output model metrics.","title":"Assignment 5"},{"location":"lessons/12-week/week05/#assignment-51","text":"Implement the movie review classifier found in section 3.4 of Deep Learning with Python .","title":"Assignment 5.1"},{"location":"lessons/12-week/week05/#assignment-52","text":"Implement the news classifier found in section 3.5 of Deep Learning with Python .","title":"Assignment 5.2"},{"location":"lessons/12-week/week05/#assignment-53","text":"Implement the housing price regression model found in section 3.6 of Deep Learning with Python .","title":"Assignment 5.3"},{"location":"lessons/12-week/week05/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment05/ directory. Use the naming convention of assignment05_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment05_DoeJane.zip assignment05 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment05 -DestinationPath ' assignment05_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week05/#discussion-board","text":"For this discussion, write a 250 to 750-word discussion board post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests. In this use case, how often would you need to train the models? How would you deploy the models? Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/week06/","text":"Week 6 \u00b6 In this lesson, you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection. Objectives \u00b6 After completing this week, you should be able to: Build a ConvNet from labeled image data to perform multiple category image classification Understand how to use existing models to classify images Describe how to fine-tune existing models for specific classification tasks Readings \u00b6 Read chapter 5 in Deep Learning with Python Weekly Resources \u00b6 CIFAR-10 DataSet Common Objects in Context COCO COCO Dataset TensorFlow Image Captioning TensorFlow Transfer Learning You Only Look Once: Unified, Real-Time Object Detection Assignment 6 \u00b6 Assignment 6.1 \u00b6 Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook. Assignment 6.2 \u00b6 Assignment 6.2.a \u00b6 Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook. Assignment 6.2.b \u00b6 Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook. Assignment 6.3 \u00b6 Load the ResNet50 model. Perform image classification on five to ten images of your choice. They can be personal images or publically available images. Include the images in dsc650/assignments/assignment06/images/ . Save the predictions dsc650/assignments/assignment06/results/predictions/resnet50 directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment06/ directory. Use the naming convention of assignment06_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment06_DoeJane.zip assignment06 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment06 -DestinationPath ' assignment06_DoeJane.zip Discussion Board \u00b6 In this lesson, we focused on using ConvNets to classify entire images. In real-world use cases, we often want to perform different tasks such as object detection, image captioning, or face detection. For this discussion, pick one of the three topics below and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 - Transfer Learning \u00b6 Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model. How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch? Topic 2 - Object detection \u00b6 In this lesson, you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image. What use cases do you see for object detection? What techniques would you use to perform object detection? Topic 3 - Face Detection \u00b6 Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data?","title":"Week 6"},{"location":"lessons/12-week/week06/#week-6","text":"In this lesson, you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection.","title":"Week 6"},{"location":"lessons/12-week/week06/#objectives","text":"After completing this week, you should be able to: Build a ConvNet from labeled image data to perform multiple category image classification Understand how to use existing models to classify images Describe how to fine-tune existing models for specific classification tasks","title":"Objectives"},{"location":"lessons/12-week/week06/#readings","text":"Read chapter 5 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week06/#weekly-resources","text":"CIFAR-10 DataSet Common Objects in Context COCO COCO Dataset TensorFlow Image Captioning TensorFlow Transfer Learning You Only Look Once: Unified, Real-Time Object Detection","title":"Weekly Resources"},{"location":"lessons/12-week/week06/#assignment-6","text":"","title":"Assignment 6"},{"location":"lessons/12-week/week06/#assignment-61","text":"Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.","title":"Assignment 6.1"},{"location":"lessons/12-week/week06/#assignment-62","text":"","title":"Assignment 6.2"},{"location":"lessons/12-week/week06/#assignment-62a","text":"Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.","title":"Assignment 6.2.a"},{"location":"lessons/12-week/week06/#assignment-62b","text":"Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset . This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.","title":"Assignment 6.2.b"},{"location":"lessons/12-week/week06/#assignment-63","text":"Load the ResNet50 model. Perform image classification on five to ten images of your choice. They can be personal images or publically available images. Include the images in dsc650/assignments/assignment06/images/ . Save the predictions dsc650/assignments/assignment06/results/predictions/resnet50 directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.","title":"Assignment 6.3"},{"location":"lessons/12-week/week06/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment06/ directory. Use the naming convention of assignment06_LastnameFirstname.zip for the zip archive. If you are using Jupyter, you can create a zip archive by running the Package Assignments.ipynb notebook. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment06_DoeJane.zip assignment06 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment06 -DestinationPath ' assignment06_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week06/#discussion-board","text":"In this lesson, we focused on using ConvNets to classify entire images. In real-world use cases, we often want to perform different tasks such as object detection, image captioning, or face detection. For this discussion, pick one of the three topics below and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/week06/#topic-1-transfer-learning","text":"Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model. How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch?","title":"Topic 1 - Transfer Learning"},{"location":"lessons/12-week/week06/#topic-2-object-detection","text":"In this lesson, you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image. What use cases do you see for object detection? What techniques would you use to perform object detection?","title":"Topic 2 - Object detection"},{"location":"lessons/12-week/week06/#topic-3-face-detection","text":"Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data?","title":"Topic 3 - Face Detection"},{"location":"lessons/12-week/week07/","text":"Week 7 \u00b6 In previous lessons, we covered how to encode data in different formats and the basics of different query languages. Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data. Objectives \u00b6 After completing this week, you should be able to: Compare and contrast different data replication strategies and discuss their advantages and disadvantages Implement basic data partitioning paradigms using Python and Parquet Describe how partitioning and replication affects data queries Readings \u00b6 Read chapters 5 and 6 in Designing Data-Intensive Applications Weekly Resources \u00b6 Cassandra HDFS Architecture YARN Architecture Assignment 7 \u00b6 Assignment 7.1 \u00b6 In this part of the assignment, you will partition a dataset using different strategies. You will use the routes.parquet dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline. For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of OMADENAA . a. \u00b6 Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called key . For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment. The partitions are determined by the first letter of the composite key using the following partitions. partitions = ( ( 'A' , 'A' ), ( 'B' , 'B' ), ( 'C' , 'D' ), ( 'E' , 'F' ), ( 'G' , 'H' ), ( 'I' , 'J' ), ( 'K' , 'L' ), ( 'M' , 'M' ), ( 'N' , 'N' ), ( 'O' , 'P' ), ( 'Q' , 'R' ), ( 'S' , 'T' ), ( 'U' , 'U' ), ( 'V' , 'V' ), ( 'W' , 'X' ), ( 'Y' , 'Z' ) ) In this case ('A', 'A') means the folder should contain all of the routes whose composite key starts with A . Similarly, ('E', 'F') should contain routes whose composite key starts with E or F . The results/kv directory should contain the following folders. kv \u251c\u2500\u2500 kv_key = A \u251c\u2500\u2500 kv_key = B \u251c\u2500\u2500 kv_key = C-D \u251c\u2500\u2500 kv_key = E-F \u251c\u2500\u2500 kv_key = G-H \u251c\u2500\u2500 kv_key = I-J \u251c\u2500\u2500 kv_key = K-L \u251c\u2500\u2500 kv_key = M \u251c\u2500\u2500 kv_key = N \u251c\u2500\u2500 kv_key = O-P \u251c\u2500\u2500 kv_key = Q-R \u251c\u2500\u2500 kv_key = S-T \u251c\u2500\u2500 kv_key = U \u251c\u2500\u2500 kv_key = V \u251c\u2500\u2500 kv_key = W-X \u2514\u2500\u2500 kv_key = Y-Z An easy way to create this directory structure is to create a new key called kv_key from the key column and use the to_parquet method with partition_cols=['kv_key'] to save a partitioned dataset. b. \u00b6 Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key. The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. import hashlib def hash_key ( key ): m = hashlib . sha256 () m . update ( str ( key ) . encode ( 'utf-8' )) return m . hexdigest () We will partition the data using the first character of the hexadecimal hash. As such, there are 16 possible partitions. Create a new column called hashed that is a hashed value of the key column. Next, create a partitioned dataset based on the first character of the hashed key and save the results to results/hash . The directory should contain the following folders. hash \u251c\u2500\u2500 hash_key = 0 \u251c\u2500\u2500 hash_key = 1 \u251c\u2500\u2500 hash_key = 2 \u251c\u2500\u2500 hash_key = 3 \u251c\u2500\u2500 hash_key = 4 \u251c\u2500\u2500 hash_key = 5 \u251c\u2500\u2500 hash_key = 6 \u251c\u2500\u2500 hash_key = 7 \u251c\u2500\u2500 hash_key = 8 \u251c\u2500\u2500 hash_key = 9 \u251c\u2500\u2500 hash_key = A \u251c\u2500\u2500 hash_key = B \u251c\u2500\u2500 hash_key = C \u251c\u2500\u2500 hash_key = D \u251c\u2500\u2500 hash_key = E c. \u00b6 Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States. Google lists the locations of their data centers and we will use the following locations for our three data centers. West The Dalles, Oregon Latitude: 45.5945645 Longitude: -121.1786823 Central Papillion, NE Latitude: 41.1544433 Longitude: -96.0422378 East Loudoun County, Virginia Latitude: 39.08344 Longitude: -77.6497145 Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport. The output folders should look as follows. geo \u251c\u2500\u2500 location = central \u251c\u2500\u2500 location = east \u2514\u2500\u2500 location = west d. \u00b6 Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition. In other words, the partitions should be ordered. def balance_partitions ( keys , num_partitions ): partitions = [] return partitions Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment07/ directory. Use the naming convention of assignment07_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment07_DoeJane.zip assignment07 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment07 -DestinationPath ' assignment07_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Compare and contrast the different replication and partitioning strategies used by different databases. Examples include HBase, Cassandra, PostgreSQL, and DynamoDB. What are the advantages and disadvantages associated with each strategy? What use cases are best suited for each paradigm? Topic 2 \u00b6 Apache Zookeeper is a key component of many big data applications. Provide examples of Zookeeper use cases. How does Zookeeper compare to etcd ? Topic 3 \u00b6 Provide a specific example of how HBase uses key-range partitioning to speed up data queries. Describe a typical query pattern for HBase.","title":"Week 7"},{"location":"lessons/12-week/week07/#week-7","text":"In previous lessons, we covered how to encode data in different formats and the basics of different query languages. Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data.","title":"Week 7"},{"location":"lessons/12-week/week07/#objectives","text":"After completing this week, you should be able to: Compare and contrast different data replication strategies and discuss their advantages and disadvantages Implement basic data partitioning paradigms using Python and Parquet Describe how partitioning and replication affects data queries","title":"Objectives"},{"location":"lessons/12-week/week07/#readings","text":"Read chapters 5 and 6 in Designing Data-Intensive Applications","title":"Readings"},{"location":"lessons/12-week/week07/#weekly-resources","text":"Cassandra HDFS Architecture YARN Architecture","title":"Weekly Resources"},{"location":"lessons/12-week/week07/#assignment-7","text":"","title":"Assignment 7"},{"location":"lessons/12-week/week07/#assignment-71","text":"In this part of the assignment, you will partition a dataset using different strategies. You will use the routes.parquet dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline. For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of OMADENAA .","title":"Assignment 7.1"},{"location":"lessons/12-week/week07/#a","text":"Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called key . For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment. The partitions are determined by the first letter of the composite key using the following partitions. partitions = ( ( 'A' , 'A' ), ( 'B' , 'B' ), ( 'C' , 'D' ), ( 'E' , 'F' ), ( 'G' , 'H' ), ( 'I' , 'J' ), ( 'K' , 'L' ), ( 'M' , 'M' ), ( 'N' , 'N' ), ( 'O' , 'P' ), ( 'Q' , 'R' ), ( 'S' , 'T' ), ( 'U' , 'U' ), ( 'V' , 'V' ), ( 'W' , 'X' ), ( 'Y' , 'Z' ) ) In this case ('A', 'A') means the folder should contain all of the routes whose composite key starts with A . Similarly, ('E', 'F') should contain routes whose composite key starts with E or F . The results/kv directory should contain the following folders. kv \u251c\u2500\u2500 kv_key = A \u251c\u2500\u2500 kv_key = B \u251c\u2500\u2500 kv_key = C-D \u251c\u2500\u2500 kv_key = E-F \u251c\u2500\u2500 kv_key = G-H \u251c\u2500\u2500 kv_key = I-J \u251c\u2500\u2500 kv_key = K-L \u251c\u2500\u2500 kv_key = M \u251c\u2500\u2500 kv_key = N \u251c\u2500\u2500 kv_key = O-P \u251c\u2500\u2500 kv_key = Q-R \u251c\u2500\u2500 kv_key = S-T \u251c\u2500\u2500 kv_key = U \u251c\u2500\u2500 kv_key = V \u251c\u2500\u2500 kv_key = W-X \u2514\u2500\u2500 kv_key = Y-Z An easy way to create this directory structure is to create a new key called kv_key from the key column and use the to_parquet method with partition_cols=['kv_key'] to save a partitioned dataset.","title":"a."},{"location":"lessons/12-week/week07/#b","text":"Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key. The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. import hashlib def hash_key ( key ): m = hashlib . sha256 () m . update ( str ( key ) . encode ( 'utf-8' )) return m . hexdigest () We will partition the data using the first character of the hexadecimal hash. As such, there are 16 possible partitions. Create a new column called hashed that is a hashed value of the key column. Next, create a partitioned dataset based on the first character of the hashed key and save the results to results/hash . The directory should contain the following folders. hash \u251c\u2500\u2500 hash_key = 0 \u251c\u2500\u2500 hash_key = 1 \u251c\u2500\u2500 hash_key = 2 \u251c\u2500\u2500 hash_key = 3 \u251c\u2500\u2500 hash_key = 4 \u251c\u2500\u2500 hash_key = 5 \u251c\u2500\u2500 hash_key = 6 \u251c\u2500\u2500 hash_key = 7 \u251c\u2500\u2500 hash_key = 8 \u251c\u2500\u2500 hash_key = 9 \u251c\u2500\u2500 hash_key = A \u251c\u2500\u2500 hash_key = B \u251c\u2500\u2500 hash_key = C \u251c\u2500\u2500 hash_key = D \u251c\u2500\u2500 hash_key = E","title":"b."},{"location":"lessons/12-week/week07/#c","text":"Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States. Google lists the locations of their data centers and we will use the following locations for our three data centers. West The Dalles, Oregon Latitude: 45.5945645 Longitude: -121.1786823 Central Papillion, NE Latitude: 41.1544433 Longitude: -96.0422378 East Loudoun County, Virginia Latitude: 39.08344 Longitude: -77.6497145 Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport. The output folders should look as follows. geo \u251c\u2500\u2500 location = central \u251c\u2500\u2500 location = east \u2514\u2500\u2500 location = west","title":"c."},{"location":"lessons/12-week/week07/#d","text":"Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition. In other words, the partitions should be ordered. def balance_partitions ( keys , num_partitions ): partitions = [] return partitions","title":"d."},{"location":"lessons/12-week/week07/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment07/ directory. Use the naming convention of assignment07_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment07_DoeJane.zip assignment07 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment07 -DestinationPath ' assignment07_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week07/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/week07/#topic-1","text":"Compare and contrast the different replication and partitioning strategies used by different databases. Examples include HBase, Cassandra, PostgreSQL, and DynamoDB. What are the advantages and disadvantages associated with each strategy? What use cases are best suited for each paradigm?","title":"Topic 1"},{"location":"lessons/12-week/week07/#topic-2","text":"Apache Zookeeper is a key component of many big data applications. Provide examples of Zookeeper use cases. How does Zookeeper compare to etcd ?","title":"Topic 2"},{"location":"lessons/12-week/week07/#topic-3","text":"Provide a specific example of how HBase uses key-range partitioning to speed up data queries. Describe a typical query pattern for HBase.","title":"Topic 3"},{"location":"lessons/12-week/week08/","text":"Week 8 \u00b6 In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. Objectives \u00b6 After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs Readings \u00b6 Read chapters 7 and 9 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications Read Kafka Use Cases Read The Log: What every software engineer should know about real-time data's unifying abstraction Weekly Resources \u00b6 etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer (REST) Spark Structured Streaming Zookeeper Assignment 8 \u00b6 For this assignment, we will be using data from the Berkeley Deep Drive . We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming . The data/processed/bdd/ folder contains the processed data for this assignment. The accelerations folder contains accelerometer data collected from each car and the locations contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. bdd \u251c\u2500\u2500 accelerations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u2502 \u251c\u2500\u2500 1 . . . \u2514\u2500\u2500 locations \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet . . . \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u251c\u2500\u2500 t = 128 .0 \u2502 \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet \u2502 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u2502 \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u2514\u2500\u2500 t = 128 .8 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet In this example, the folder t=000.0 is the start of the simulated data. The folder t=052.2 is 52.2 seconds into the simulation and t=128.8 is 128.8 seconds into the simulation. Assignment 8 \u00b6 The first part of the assignment involves creating a Jupyter notebook that mimics a real-time streaming data feed. The basic loop for the notebook is simple. The notebook should load processed data and publish data at the appropriate time. You can use either the time given in the parquet partition or you can use the offset data found within the parquet data. For example, once your notebook has passed the 52.5-second mark it should load the data from the t=052.5 directory and publish it to the appropriate Kafka topic. Similarly, you could example the offset column and publish the data at the appropriate time. Hint You may want to use the Python heapq library as an event queue. The DSC 650 Github contains example notebooks you can use to help you create topics, publish data to a Kafka broker, and consume the data. Use the following parameters when publishing simulated data to the Bellevue University Data Science Cluster Kafka broker. Bootstrap Server kafka.kafka.svc.cluster.local:9092 Location Topic LastnameFirstname-locations Acceleration Topic LastnameFirstname-accelerations The following code is an example of code that uses the kafka-python library to publish a message to Kafka topic using a JSON serializer. import json from kafka import KafkaProducer bootstrap_server = 'kafka.kafka.svc.cluster.local:9092' producer = KafkaProducer ( bootstrap_servers = [ bootstrap_server ], value_serializer = lambda x : json . dumps ( x ) . encode ( 'utf-8' ) ) producer . send ( 'DoeJohn-locations' , { \"dataObjectID\" : \"test1\" } ) Hint When creating the notebook producer, you may want to automatically restart sending the data from the beginning when you reach the end of the dataset. This enables you to continue testing without having to manually restart the notebook. The following code is an example that uses the kafka-python library to consume messages from a Kafka topic. You should create another Jupyter notebook to consume messages from the Kafka producer to validate that you are properly publishing messages to the appropriate topic. from kafka import KafkaConsumer bootstrap_server = 'kafka.kafka.svc.cluster.local:9092' # To consume latest messages and auto-commit offsets consumer = KafkaConsumer ( 'DoeJohn-locations' , bootstrap_servers = [ bootstrap_server ] ) Note While creating a separate notebook that acts as a Kafka consumer is not strictly necessary for the assignment, it is recommended that you create one to aid in debugging and testing. Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems. What other data systems make use of this data structure? Topic 2 \u00b6 Representational State Transfer (REST) is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from the client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications?","title":"Week 8"},{"location":"lessons/12-week/week08/#week-8","text":"In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems.","title":"Week 8"},{"location":"lessons/12-week/week08/#objectives","text":"After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs","title":"Objectives"},{"location":"lessons/12-week/week08/#readings","text":"Read chapters 7 and 9 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications Read Kafka Use Cases Read The Log: What every software engineer should know about real-time data's unifying abstraction","title":"Readings"},{"location":"lessons/12-week/week08/#weekly-resources","text":"etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer (REST) Spark Structured Streaming Zookeeper","title":"Weekly Resources"},{"location":"lessons/12-week/week08/#assignment-8","text":"For this assignment, we will be using data from the Berkeley Deep Drive . We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming . The data/processed/bdd/ folder contains the processed data for this assignment. The accelerations folder contains accelerometer data collected from each car and the locations contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. bdd \u251c\u2500\u2500 accelerations \u2502 \u251c\u2500\u2500 t = 000 .0 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 001 .5 \u2502 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u2502 \u251c\u2500\u2500 t = 003 .2 \u2502 \u2502 \u251c\u2500\u2500 1 . . . \u2514\u2500\u2500 locations \u251c\u2500\u2500 t = 000 .0 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u251c\u2500\u2500 t = 001 .5 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet \u2502 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet \u2502 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet \u251c\u2500\u2500 t = 003 .2 \u2502 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet \u2502 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet \u2502 \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet . . . \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u251c\u2500\u2500 t = 128 .0 \u2502 \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet \u2502 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u2502 \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2502 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet \u2514\u2500\u2500 t = 128 .8 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet In this example, the folder t=000.0 is the start of the simulated data. The folder t=052.2 is 52.2 seconds into the simulation and t=128.8 is 128.8 seconds into the simulation.","title":"Assignment 8"},{"location":"lessons/12-week/week08/#assignment-8_1","text":"The first part of the assignment involves creating a Jupyter notebook that mimics a real-time streaming data feed. The basic loop for the notebook is simple. The notebook should load processed data and publish data at the appropriate time. You can use either the time given in the parquet partition or you can use the offset data found within the parquet data. For example, once your notebook has passed the 52.5-second mark it should load the data from the t=052.5 directory and publish it to the appropriate Kafka topic. Similarly, you could example the offset column and publish the data at the appropriate time. Hint You may want to use the Python heapq library as an event queue. The DSC 650 Github contains example notebooks you can use to help you create topics, publish data to a Kafka broker, and consume the data. Use the following parameters when publishing simulated data to the Bellevue University Data Science Cluster Kafka broker. Bootstrap Server kafka.kafka.svc.cluster.local:9092 Location Topic LastnameFirstname-locations Acceleration Topic LastnameFirstname-accelerations The following code is an example of code that uses the kafka-python library to publish a message to Kafka topic using a JSON serializer. import json from kafka import KafkaProducer bootstrap_server = 'kafka.kafka.svc.cluster.local:9092' producer = KafkaProducer ( bootstrap_servers = [ bootstrap_server ], value_serializer = lambda x : json . dumps ( x ) . encode ( 'utf-8' ) ) producer . send ( 'DoeJohn-locations' , { \"dataObjectID\" : \"test1\" } ) Hint When creating the notebook producer, you may want to automatically restart sending the data from the beginning when you reach the end of the dataset. This enables you to continue testing without having to manually restart the notebook. The following code is an example that uses the kafka-python library to consume messages from a Kafka topic. You should create another Jupyter notebook to consume messages from the Kafka producer to validate that you are properly publishing messages to the appropriate topic. from kafka import KafkaConsumer bootstrap_server = 'kafka.kafka.svc.cluster.local:9092' # To consume latest messages and auto-commit offsets consumer = KafkaConsumer ( 'DoeJohn-locations' , bootstrap_servers = [ bootstrap_server ] ) Note While creating a separate notebook that acts as a Kafka consumer is not strictly necessary for the assignment, it is recommended that you create one to aid in debugging and testing.","title":"Assignment 8"},{"location":"lessons/12-week/week08/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/week08/#topic-1","text":"Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems. What other data systems make use of this data structure?","title":"Topic 1"},{"location":"lessons/12-week/week08/#topic-2","text":"Representational State Transfer (REST) is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from the client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications?","title":"Topic 2"},{"location":"lessons/12-week/week09/","text":"Week 9 \u00b6 In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. Objectives \u00b6 After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs Readings \u00b6 Read chapter 11 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications Weekly Resources \u00b6 etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer (REST) Spark Structured Streaming Zookeeper Assignment 9 \u00b6 In the second part of the exercise, you will create two streaming dataframes using the accelerations and locations folders. Assignment 9.1 \u00b6 Start by creating a simple Spark Streaming application that reads data from the accelerations and locations topics and uses the Kafka sink to save the results to the LastnameFirstname-simple topic. Assignment 9.2 \u00b6 Define a watermark on the locations dataframe using the timestamp column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each ride defined by the ride_id . Save the results in LastnameFirstname-windowed and set the output mode to update . Assignment 9.3 \u00b6 Join the two streams together on the ride_id as an inner join. Save the results in LastnameFirstname-joined . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment09/ directory. Use the naming convention of assignment09_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment09_DoeJane.zip assignment08 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment09 -DestinationPath ' assignment09_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Describe how different database systems handle transactions. Pick three or more different systems to compare and contrast.","title":"Week 9"},{"location":"lessons/12-week/week09/#week-9","text":"In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems.","title":"Week 9"},{"location":"lessons/12-week/week09/#objectives","text":"After completing this week, you should be able to: Implement scalable stream processing in Spark Explain different approaches to transactions in distributed systems and the associated trade-offs","title":"Objectives"},{"location":"lessons/12-week/week09/#readings","text":"Read chapter 11 in Designing Data-Intensive Applications (Optional) Read chapters 8 in Designing Data-Intensive Applications","title":"Readings"},{"location":"lessons/12-week/week09/#weekly-resources","text":"etcd Kafka Use Cases Kafka Introduction The Log: What every software engineer should know about real-time data's unifying abstraction RabbitMQ Semantics Representational State Transfer (REST) Spark Structured Streaming Zookeeper","title":"Weekly Resources"},{"location":"lessons/12-week/week09/#assignment-9","text":"In the second part of the exercise, you will create two streaming dataframes using the accelerations and locations folders.","title":"Assignment 9"},{"location":"lessons/12-week/week09/#assignment-91","text":"Start by creating a simple Spark Streaming application that reads data from the accelerations and locations topics and uses the Kafka sink to save the results to the LastnameFirstname-simple topic.","title":"Assignment 9.1"},{"location":"lessons/12-week/week09/#assignment-92","text":"Define a watermark on the locations dataframe using the timestamp column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each ride defined by the ride_id . Save the results in LastnameFirstname-windowed and set the output mode to update .","title":"Assignment 9.2"},{"location":"lessons/12-week/week09/#assignment-93","text":"Join the two streams together on the ride_id as an inner join. Save the results in LastnameFirstname-joined .","title":"Assignment 9.3"},{"location":"lessons/12-week/week09/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment09/ directory. Use the naming convention of assignment09_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment09_DoeJane.zip assignment08 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment09 -DestinationPath ' assignment09_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week09/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Describe how different database systems handle transactions. Pick three or more different systems to compare and contrast.","title":"Discussion Board"},{"location":"lessons/12-week/week10/","text":"Week 10 \u00b6 In this lesson we learn how to preprocess text-based data and train deep learning models on that data. Objectives \u00b6 After completing this week, you should be able to: Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing. Build basic text-processing models using recurrent neural networks (RNN) Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models Readings \u00b6 Read chapter 6 in Deep Learning with Python Weekly Resources \u00b6 Global Vectors for Word Representation Large Movie Review Dataset Extracting, transforming and selecting features Assignment 10 \u00b6 Assignment 10.1 \u00b6 In the first part of the assignment, you will implement basic text-preprocessing functions in Python. These functions do not need to scale to large text documents and will only need to handle small inputs. Assignment 10.1.a \u00b6 Create a tokenize function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. def tokenize ( sentence ): tokens = [] # tokenize the sentence return tokens ```` #### Assignment 10.1.b Implement an ` ngram ` function that splits tokens into N - grams . ``` python def ngram ( tokens , n ): ngrams = [] # Create ngrams return ngrams Assignment 10.1.c \u00b6 Implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens. def one_hot_encode ( tokens , num_words ): token_index = {} results = '' return results 10.2 \u00b6 Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in data/external/imdb/ . Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook. 10.3 \u00b6 Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook. 10.4 \u00b6 Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment10/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment09 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment10_DoeJane.zip Discussion Board \u00b6 For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. Topic 1 \u00b6 Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another? Topic 2 \u00b6 How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests.","title":"Week 10"},{"location":"lessons/12-week/week10/#week-10","text":"In this lesson we learn how to preprocess text-based data and train deep learning models on that data.","title":"Week 10"},{"location":"lessons/12-week/week10/#objectives","text":"After completing this week, you should be able to: Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing. Build basic text-processing models using recurrent neural networks (RNN) Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models","title":"Objectives"},{"location":"lessons/12-week/week10/#readings","text":"Read chapter 6 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week10/#weekly-resources","text":"Global Vectors for Word Representation Large Movie Review Dataset Extracting, transforming and selecting features","title":"Weekly Resources"},{"location":"lessons/12-week/week10/#assignment-10","text":"","title":"Assignment 10"},{"location":"lessons/12-week/week10/#assignment-101","text":"In the first part of the assignment, you will implement basic text-preprocessing functions in Python. These functions do not need to scale to large text documents and will only need to handle small inputs.","title":"Assignment 10.1"},{"location":"lessons/12-week/week10/#assignment-101a","text":"Create a tokenize function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. def tokenize ( sentence ): tokens = [] # tokenize the sentence return tokens ```` #### Assignment 10.1.b Implement an ` ngram ` function that splits tokens into N - grams . ``` python def ngram ( tokens , n ): ngrams = [] # Create ngrams return ngrams","title":"Assignment 10.1.a"},{"location":"lessons/12-week/week10/#assignment-101c","text":"Implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens. def one_hot_encode ( tokens , num_words ): token_index = {} results = '' return results","title":"Assignment 10.1.c"},{"location":"lessons/12-week/week10/#102","text":"Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in data/external/imdb/ . Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.","title":"10.2"},{"location":"lessons/12-week/week10/#103","text":"Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.","title":"10.3"},{"location":"lessons/12-week/week10/#104","text":"Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.","title":"10.4"},{"location":"lessons/12-week/week10/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment10/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment09 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment10_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week10/#discussion-board","text":"For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies. For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board.","title":"Discussion Board"},{"location":"lessons/12-week/week10/#topic-1","text":"Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another?","title":"Topic 1"},{"location":"lessons/12-week/week10/#topic-2","text":"How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests.","title":"Topic 2"},{"location":"lessons/12-week/week11/","text":"Week 11 \u00b6 In this lesson, we will explore the future of big data and deep learning. Objectives \u00b6 After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation Readings \u00b6 Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python Weekly Resources \u00b6 Assignment 11 \u00b6 Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the results directory of dsc650/assignments/assignment11/ . Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment11/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment11 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment11_DoeJane.zip Discussion Board \u00b6 For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic.","title":"Week 11"},{"location":"lessons/12-week/week11/#week-11","text":"In this lesson, we will explore the future of big data and deep learning.","title":"Week 11"},{"location":"lessons/12-week/week11/#objectives","text":"After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation","title":"Objectives"},{"location":"lessons/12-week/week11/#readings","text":"Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week11/#weekly-resources","text":"","title":"Weekly Resources"},{"location":"lessons/12-week/week11/#assignment-11","text":"Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the results directory of dsc650/assignments/assignment11/ .","title":"Assignment 11"},{"location":"lessons/12-week/week11/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment11/ directory. Use the naming convention of assignment10_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment10_DoeJane.zip assignment11 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment11_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week11/#discussion-board","text":"For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic.","title":"Discussion Board"},{"location":"lessons/12-week/week12/","text":"Week 12 \u00b6 In this lesson, we will explore the future of big data and deep learning. Objectives \u00b6 After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation Readings \u00b6 Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python Weekly Resources \u00b6 Assignment 12 \u00b6 Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the results/vae directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead. Submission Instructions \u00b6 For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment12/ directory. Use the naming convention of assignment12_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment12_DoeJane.zip assignment10 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment12_DoeJane.zip Discussion Board \u00b6 For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic. Use the DSC 650 Slack channel for discussion and replies.","title":"Week 12"},{"location":"lessons/12-week/week12/#week-12","text":"In this lesson, we will explore the future of big data and deep learning.","title":"Week 12"},{"location":"lessons/12-week/week12/#objectives","text":"After completing this week, you should be able to: Describe upcoming advances in big data and deep learning and their potential use cases Experiment with advanced deep learning use cases including text and image generation","title":"Objectives"},{"location":"lessons/12-week/week12/#readings","text":"Chapter 12 in Designing Data-Intensive Applications Read chapters 8 and 9 in Deep Learning with Python","title":"Readings"},{"location":"lessons/12-week/week12/#weekly-resources","text":"","title":"Weekly Resources"},{"location":"lessons/12-week/week12/#assignment-12","text":"Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the results/vae directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead.","title":"Assignment 12"},{"location":"lessons/12-week/week12/#submission-instructions","text":"For this assignment, you will submit a zip archive containing the contents of the dsc650/assignments/assignment12/ directory. Use the naming convention of assignment12_LastnameFirstname.zip for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. cd dsc650/assignments zip -r assignment12_DoeJane.zip assignment10 Likewise, you can create a zip archive using Windows PowerShell with the following command. Compress-Archive -Path assignment10 -DestinationPath ' assignment12_DoeJane.zip","title":"Submission Instructions"},{"location":"lessons/12-week/week12/#discussion-board","text":"For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic. Use the DSC 650 Slack channel for discussion and replies.","title":"Discussion Board"},{"location":"lessons/fundamentals/latency/","text":"Latency \u00b6 Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency"},{"location":"lessons/fundamentals/latency/#latency","text":"Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency"},{"location":"lessons/fundamentals/size/","text":"Data Size \u00b6 Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Size"},{"location":"lessons/fundamentals/size/#data-size","text":"Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"setup/","text":"Overview \u00b6 Note It is recommended that you use the hosted option for this class. Operating System Dependencies \u00b6 See the pages hosted , macOS , Ubuntu , or Windows 10 for how to install the operating system specific dependencies for your computer. Clone Github Repository \u00b6 Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location.","title":"Overview"},{"location":"setup/#overview","text":"Note It is recommended that you use the hosted option for this class.","title":"Overview"},{"location":"setup/#operating-system-dependencies","text":"See the pages hosted , macOS , Ubuntu , or Windows 10 for how to install the operating system specific dependencies for your computer.","title":"Operating System Dependencies"},{"location":"setup/#clone-github-repository","text":"Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location.","title":"Clone Github Repository"},{"location":"setup/hosted/","text":"Hosted Environments \u00b6 Bellevue University Data Science Cluster \u00b6 Bellevue University hosts a JupyterHub instance that includes TensorFlow, PySpark, R, and other data science libraries. To access JupyterHub, start by going to https://workspace.bellevue.edu/ . Download and install the VMware Horizon Client. Once you have installed the client, select the add server option, and add workspace.bellevue.edu . After you add the server, login to the client using your Bellevue University username and password. Once you log in, you should see multiple desktops which should include the DSC Desktop . Select the DSC Desktop option which should take you to a Windows virtual machine that is pre-configured to use the JupyterHub instance. Within the DSC Desktop instance, open either Firefox or Chrome. If the homepage doesn't automatically take you to the Bellevue Data Science Cluster homepage, you can manually navigate to it at http://home.budsc.midwest-datascience.com/ . This homepage is the landing page for Data Science Cluster applications. Currently, JupyterHub is the only capability available to students, but more capabilities will be available in the future. To access JupyterHub instance , click on the JupyterHub logo You will be asked to log in using your Github account. You can use whatever Github account you would like. Once you log in, you should be taken to a JupyterHub notebook. This notebook is backed by persistent storage with 50GB. Google Colaboratory \u00b6 Colaboratory , or \"Colab\" for short, provides hosted Jupyter notebooks with free access to GPUs and Tensorflow. Databricks Community Edition \u00b6 Databricks Community Edition provides free access to notebooks configured with PySpark.","title":"Hosted"},{"location":"setup/hosted/#hosted-environments","text":"","title":"Hosted Environments"},{"location":"setup/hosted/#bellevue-university-data-science-cluster","text":"Bellevue University hosts a JupyterHub instance that includes TensorFlow, PySpark, R, and other data science libraries. To access JupyterHub, start by going to https://workspace.bellevue.edu/ . Download and install the VMware Horizon Client. Once you have installed the client, select the add server option, and add workspace.bellevue.edu . After you add the server, login to the client using your Bellevue University username and password. Once you log in, you should see multiple desktops which should include the DSC Desktop . Select the DSC Desktop option which should take you to a Windows virtual machine that is pre-configured to use the JupyterHub instance. Within the DSC Desktop instance, open either Firefox or Chrome. If the homepage doesn't automatically take you to the Bellevue Data Science Cluster homepage, you can manually navigate to it at http://home.budsc.midwest-datascience.com/ . This homepage is the landing page for Data Science Cluster applications. Currently, JupyterHub is the only capability available to students, but more capabilities will be available in the future. To access JupyterHub instance , click on the JupyterHub logo You will be asked to log in using your Github account. You can use whatever Github account you would like. Once you log in, you should be taken to a JupyterHub notebook. This notebook is backed by persistent storage with 50GB.","title":"Bellevue University Data Science Cluster"},{"location":"setup/hosted/#google-colaboratory","text":"Colaboratory , or \"Colab\" for short, provides hosted Jupyter notebooks with free access to GPUs and Tensorflow.","title":"Google Colaboratory"},{"location":"setup/hosted/#databricks-community-edition","text":"Databricks Community Edition provides free access to notebooks configured with PySpark.","title":"Databricks Community Edition"},{"location":"setup/macOS/","text":"macOS \u00b6 Documentation in Progress Check back soon for more updates. Package Manager \u00b6 If you are using macOS as your primary development environment, I recommend using a package manager like homebrew . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Chocolatey is a popular package manager for Windows. You can install Homebrew on your system by executing the following command in your terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" I also recommend using Homebrew Cask to install graphical applications like Atom and Google Chrome. After you have installed Homebrew, update your package index. brew update Finally, install the following packages required for this course. brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install libtensorflow brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy Optionally, you can install the following packages using Homebrew Cask. brew cask install anaconda brew cask install atom brew cask install github brew cask install mactex brew cask install miniconda brew cask install virtualbox JDK \u00b6 Spark and Hadoop use version 8 of the Java Development Kit (JDK 8). Download the latest version from Oracle and install on your local machine. Once completed, edit your shell profile \u2013 $HOME/.bash_profile if you are using Bash or $HOME/.zshrc if you are using Zsh \u2013 and add the following. export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home TensorFlow \u00b6 Note There is no GPU support for macOS","title":"macOS"},{"location":"setup/macOS/#macos","text":"Documentation in Progress Check back soon for more updates.","title":"macOS"},{"location":"setup/macOS/#package-manager","text":"If you are using macOS as your primary development environment, I recommend using a package manager like homebrew . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Chocolatey is a popular package manager for Windows. You can install Homebrew on your system by executing the following command in your terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" I also recommend using Homebrew Cask to install graphical applications like Atom and Google Chrome. After you have installed Homebrew, update your package index. brew update Finally, install the following packages required for this course. brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install libtensorflow brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy Optionally, you can install the following packages using Homebrew Cask. brew cask install anaconda brew cask install atom brew cask install github brew cask install mactex brew cask install miniconda brew cask install virtualbox","title":"Package Manager"},{"location":"setup/macOS/#jdk","text":"Spark and Hadoop use version 8 of the Java Development Kit (JDK 8). Download the latest version from Oracle and install on your local machine. Once completed, edit your shell profile \u2013 $HOME/.bash_profile if you are using Bash or $HOME/.zshrc if you are using Zsh \u2013 and add the following. export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home","title":"JDK"},{"location":"setup/macOS/#tensorflow","text":"Note There is no GPU support for macOS","title":"TensorFlow"},{"location":"setup/ubuntu/","text":"Ubuntu \u00b6 Documentation in Progress Check back soon for more updates. System \u00b6 Edit /etc/hosts : 127 .0.0.1 hostname Install JDK, Scala and Git : sudo apt install default-jdk scala git -y Install Poetry : curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3 Install Oracle JDK : sudo apt update sudo add-apt-repository ppa:webupd8team/java sudo apt update sudo apt install oracle-java8-installer oracle-java8-set-default Spark \u00b6 This guide provides more information on how to setup Spark on Ubuntu. Start by downloading Spark 2.4.5 for Hadoop 2.7. curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz Extract the archive. tar xvf spark-2.4.5-bin-hadoop2.7.tgz Move it to /opt/spark . sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark Update the environment variables by adding the following to your shell profile. export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin export PYSPARK_PYTHON = /usr/bin/python3 Alternatively, add it to your profile using echo . echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile echo \"export PATH= $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin\" >> ~/.profile echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile This assumes your profile is in .profile . It may also be in another location like ~/.bashrc or ~/.zshrc . Activate your changes as follows. source ~/.bashrc Start a stand-alone server. start-master.sh The process will listen on 8080. ss -tunelp | grep 8080 tcp LISTEN 0 1 *:8080 Start a worker process. start-slave.sh spark://ubuntu:7077 You can stop the processes using the following commands. stop-slave.sh stop-master.sh TensorFlow \u00b6 Ubuntu 18.04 ships with Python 3 by default sudo apt install python3-venv Note If you have a dedicated NVIDIA GPU and want to take advantage of its processing power, instead of tensorflow install the tensorflow-gpu package which includes GPU support. GPU Support \u00b6 Check the following links to more information on GPU support. CUDA GPUs TensorFlow GPU Install # Add NVIDIA package repositories wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-get update wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt-get update # Install NVIDIA driver sudo apt-get install --no-install-recommends nvidia-driver-430 # Reboot. Check that GPUs are visible using the command: nvidia-smi # Install development and runtime libraries (~4GB) sudo apt-get install --no-install-recommends \\ cuda-10-1 \\ libcudnn7 = 7 .6.4.38-1+cuda10.1 \\ libcudnn7-dev = 7 .6.4.38-1+cuda10.1 # Install TensorRT. Requires that libcudnn7 is installed above. sudo apt-get install -y --no-install-recommends libnvinfer6 = 6 .0.1-1+cuda10.1 \\ libnvinfer-dev = 6 .0.1-1+cuda10.1 \\ libnvinfer-plugin6 = 6 .0.1-1+cuda10.1","title":"Ubuntu"},{"location":"setup/ubuntu/#ubuntu","text":"Documentation in Progress Check back soon for more updates.","title":"Ubuntu"},{"location":"setup/ubuntu/#system","text":"Edit /etc/hosts : 127 .0.0.1 hostname Install JDK, Scala and Git : sudo apt install default-jdk scala git -y Install Poetry : curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3 Install Oracle JDK : sudo apt update sudo add-apt-repository ppa:webupd8team/java sudo apt update sudo apt install oracle-java8-installer oracle-java8-set-default","title":"System"},{"location":"setup/ubuntu/#spark","text":"This guide provides more information on how to setup Spark on Ubuntu. Start by downloading Spark 2.4.5 for Hadoop 2.7. curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz Extract the archive. tar xvf spark-2.4.5-bin-hadoop2.7.tgz Move it to /opt/spark . sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark Update the environment variables by adding the following to your shell profile. export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin export PYSPARK_PYTHON = /usr/bin/python3 Alternatively, add it to your profile using echo . echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile echo \"export PATH= $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin\" >> ~/.profile echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile This assumes your profile is in .profile . It may also be in another location like ~/.bashrc or ~/.zshrc . Activate your changes as follows. source ~/.bashrc Start a stand-alone server. start-master.sh The process will listen on 8080. ss -tunelp | grep 8080 tcp LISTEN 0 1 *:8080 Start a worker process. start-slave.sh spark://ubuntu:7077 You can stop the processes using the following commands. stop-slave.sh stop-master.sh","title":"Spark"},{"location":"setup/ubuntu/#tensorflow","text":"Ubuntu 18.04 ships with Python 3 by default sudo apt install python3-venv Note If you have a dedicated NVIDIA GPU and want to take advantage of its processing power, instead of tensorflow install the tensorflow-gpu package which includes GPU support.","title":"TensorFlow"},{"location":"setup/ubuntu/#gpu-support","text":"Check the following links to more information on GPU support. CUDA GPUs TensorFlow GPU Install # Add NVIDIA package repositories wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-get update wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt-get update # Install NVIDIA driver sudo apt-get install --no-install-recommends nvidia-driver-430 # Reboot. Check that GPUs are visible using the command: nvidia-smi # Install development and runtime libraries (~4GB) sudo apt-get install --no-install-recommends \\ cuda-10-1 \\ libcudnn7 = 7 .6.4.38-1+cuda10.1 \\ libcudnn7-dev = 7 .6.4.38-1+cuda10.1 # Install TensorRT. Requires that libcudnn7 is installed above. sudo apt-get install -y --no-install-recommends libnvinfer6 = 6 .0.1-1+cuda10.1 \\ libnvinfer-dev = 6 .0.1-1+cuda10.1 \\ libnvinfer-plugin6 = 6 .0.1-1+cuda10.1","title":"GPU Support"},{"location":"setup/windows/","text":"Windows \u00b6 Documentation in Progress Check back soon for more updates. Overview \u00b6 There are multiple options available for installing Spark, Hadoop, TensorFlow, and other Big Data/Deep Learning software on Windows 10. While it is possible to install these packages and use these packages on Windows, I strongly urge you to heed the warning of Fran\u00e7ois Chollet, author of Deep Learning With Python . Whether you\u2019re running locally or in the cloud, it\u2019s better to be using a Unix workstation. Although it\u2019s technically possible to use Keras on Windows (all three Keras backends support Windows), We don\u2019t recommend it. In the installation instructions in appendix A, we\u2019ll consider an Ubuntu machine. If you\u2019re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run. Prerequisites \u00b6 Install this software prior to setting up your environment. Atom Optional Optional GitHub Desktop Git Anaconda Java Development Kit 8 PyCharm Clone GitHub Repository \u00b6 Using GitHub Desktop, clone the bellevue-university/dsc650 repository by going to https://github.com/bellevue-university/dsc650 and selecting the Open In Desktop option. Clone the repository to your local system by selecting the appropriate local directory. After selecting the directory, you will see a screen that shows the repository cloning to your local directory. This process may take a long time (minutes to hours), so wait until it is completed. Import Environment \u00b6 Next, open Anaconda Navigator. Select the channels option to add the conda-forge channel. After you have finished adding the Conda Forge channel, import a new environment by selecting the environments tab and the import option. Import the environment.yaml file from the dsc650 repository to the dsc650 environment. This will create an Anaconda environment with the appropriate dependencies. Open PyCharm Project \u00b6 Open PyCharm where you should see a screen welcoming you to Pycharm. Select the open option and open the directory where you cloned the dsc650 repository. When you initially open the project, it may ask you to fix issues with Windows Defender. Fix the issues by clicking the fix option and following the prompts. PyCharm should automatically use the previously created dsc650 environment. If not, go to the project-interpreter option in the menu and add the Conda environment. Set Sources Root \u00b6 After opening the project, right click the dsc650 directory and add it as a sources root. Set Java Home \u00b6 Variable Value JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_251 Go the edit system environment variables in your control panel. Under System Properties -> Advanced select Environment Variables . Change the environment variables for your user. Exit out of PyCharm and re-open to ensure it sets the environment variables. Run Examples \u00b6 Run the TensorFlow example. Run the PySpark example. Or you can run the PySpark example in the terminal. Package Manager (Optional) \u00b6 If you are using Windows as your primary development environment, I recommend using a package manager like Chocolatey . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Follow the Chocolatey installation guide to install the package manager on your system. Once you have completed installing the package manager, you can install new software by running PowerShell as an administrator and using the choco command. For example, the following commands will install the latest versions of Adobe Acrobat Reader, Google Chrome, and FireFox on your system. choco install adobereader choco install googlechrome choco install firefox You can upgrade all packages using choco upgrade all or upgrade individual packages using choco upgrade firefox . Similarly, you can uninstall packages using choco uninstall . The following is a table of software you might find useful for this course. Software Package Name Anaconda Distribution (Python 3.x) anaconda3 Git (Install) git.install GitHub Desktop github-desktop Graphviz graphviz Hadoop hadoop Java Development Kit 8 jdk8 JetBrains Toolbox App jetbrainstoolbox JetBrains DataGrip datagrip JetBrains PyCharm pycharm JetBrains PyCharm Educational pycharm-edu JetBrains PyCharm (Community Edition) 1 pycharm-community MikTeX miktex Pandoc pandoc Pandoc CrossRef pandoc-crossref PostgreSQL postgresql Protocol Buffers protoc Scala scala VirtualBox virtualbox If you are interested to see what other packages are available, see Chocolatey packages for a list of community maintained packages. While you can use the community version of PyCharm, JetBrains offers free educational licenses for students and teachers. See educational licenses for more details. \u21a9","title":"Windows"},{"location":"setup/windows/#windows","text":"Documentation in Progress Check back soon for more updates.","title":"Windows"},{"location":"setup/windows/#overview","text":"There are multiple options available for installing Spark, Hadoop, TensorFlow, and other Big Data/Deep Learning software on Windows 10. While it is possible to install these packages and use these packages on Windows, I strongly urge you to heed the warning of Fran\u00e7ois Chollet, author of Deep Learning With Python . Whether you\u2019re running locally or in the cloud, it\u2019s better to be using a Unix workstation. Although it\u2019s technically possible to use Keras on Windows (all three Keras backends support Windows), We don\u2019t recommend it. In the installation instructions in appendix A, we\u2019ll consider an Ubuntu machine. If you\u2019re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run.","title":"Overview"},{"location":"setup/windows/#prerequisites","text":"Install this software prior to setting up your environment. Atom Optional Optional GitHub Desktop Git Anaconda Java Development Kit 8 PyCharm","title":"Prerequisites"},{"location":"setup/windows/#clone-github-repository","text":"Using GitHub Desktop, clone the bellevue-university/dsc650 repository by going to https://github.com/bellevue-university/dsc650 and selecting the Open In Desktop option. Clone the repository to your local system by selecting the appropriate local directory. After selecting the directory, you will see a screen that shows the repository cloning to your local directory. This process may take a long time (minutes to hours), so wait until it is completed.","title":"Clone GitHub Repository"},{"location":"setup/windows/#import-environment","text":"Next, open Anaconda Navigator. Select the channels option to add the conda-forge channel. After you have finished adding the Conda Forge channel, import a new environment by selecting the environments tab and the import option. Import the environment.yaml file from the dsc650 repository to the dsc650 environment. This will create an Anaconda environment with the appropriate dependencies.","title":"Import Environment"},{"location":"setup/windows/#open-pycharm-project","text":"Open PyCharm where you should see a screen welcoming you to Pycharm. Select the open option and open the directory where you cloned the dsc650 repository. When you initially open the project, it may ask you to fix issues with Windows Defender. Fix the issues by clicking the fix option and following the prompts. PyCharm should automatically use the previously created dsc650 environment. If not, go to the project-interpreter option in the menu and add the Conda environment.","title":"Open PyCharm Project"},{"location":"setup/windows/#set-sources-root","text":"After opening the project, right click the dsc650 directory and add it as a sources root.","title":"Set Sources Root"},{"location":"setup/windows/#set-java-home","text":"Variable Value JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_251 Go the edit system environment variables in your control panel. Under System Properties -> Advanced select Environment Variables . Change the environment variables for your user. Exit out of PyCharm and re-open to ensure it sets the environment variables.","title":"Set Java Home"},{"location":"setup/windows/#run-examples","text":"Run the TensorFlow example. Run the PySpark example. Or you can run the PySpark example in the terminal.","title":"Run Examples"},{"location":"setup/windows/#package-manager-optional","text":"If you are using Windows as your primary development environment, I recommend using a package manager like Chocolatey . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Follow the Chocolatey installation guide to install the package manager on your system. Once you have completed installing the package manager, you can install new software by running PowerShell as an administrator and using the choco command. For example, the following commands will install the latest versions of Adobe Acrobat Reader, Google Chrome, and FireFox on your system. choco install adobereader choco install googlechrome choco install firefox You can upgrade all packages using choco upgrade all or upgrade individual packages using choco upgrade firefox . Similarly, you can uninstall packages using choco uninstall . The following is a table of software you might find useful for this course. Software Package Name Anaconda Distribution (Python 3.x) anaconda3 Git (Install) git.install GitHub Desktop github-desktop Graphviz graphviz Hadoop hadoop Java Development Kit 8 jdk8 JetBrains Toolbox App jetbrainstoolbox JetBrains DataGrip datagrip JetBrains PyCharm pycharm JetBrains PyCharm Educational pycharm-edu JetBrains PyCharm (Community Edition) 1 pycharm-community MikTeX miktex Pandoc pandoc Pandoc CrossRef pandoc-crossref PostgreSQL postgresql Protocol Buffers protoc Scala scala VirtualBox virtualbox If you are interested to see what other packages are available, see Chocolatey packages for a list of community maintained packages. While you can use the community version of PyCharm, JetBrains offers free educational licenses for students and teachers. See educational licenses for more details. \u21a9","title":"Package Manager (Optional)"}]}